{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Andrzej\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Andrzej\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Andrzej\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Andrzej\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Andrzej\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Andrzej\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Andrzej\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Andrzej\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Andrzej\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Andrzej\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Andrzej\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Andrzej\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, plot_confusion_matrix, accuracy_score, plot_roc_curve,\\\n",
    "                             precision_recall_curve, plot_precision_recall_curve, f1_score, average_precision_score,\\\n",
    "                             hinge_loss, precision_score, recall_score, classification_report\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate, KFold\n",
    "from sklearn.metrics import make_scorer, accuracy_score, average_precision_score, f1_score,\\\n",
    "                            log_loss, precision_score, recall_score, roc_auc_score\n",
    "from sklearn.preprocessing import label_binarize, LabelBinarizer, LabelEncoder, OneHotEncoder\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from catboost import CatBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmnist = fetch_openml(\"Fashion-MNIST\", data_home=\"./fmnist\", cache=True)\n",
    "classes = [str(x) for x in range(0, 10)]\n",
    "num_classes = len(classes)\n",
    "\n",
    "def mk_dataset(total, fmnist=fmnist, classes=classes):\n",
    "    samples = int(fmnist.data.shape[0]*total)\n",
    "    return resample(fmnist.data, fmnist.target, n_samples=samples)\n",
    "\n",
    "def plot_imgs(x, y, w=28, h=28):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for i in range(min(25, x.shape[0])):\n",
    "        plt.subplot(5,5,i+1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        img = x[i]\n",
    "        img = img.reshape((w, h))\n",
    "        plt.imshow(img)\n",
    "        plt.xlabel(y[i])\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarized_scorer(metric, **kwargs):\n",
    "    lb = LabelBinarizer()\n",
    "    def score(y_test, y_pred, metric=metric, lb=lb, kwargs=kwargs):\n",
    "        lb.fit(y_test)\n",
    "        y_test = lb.transform(y_test)\n",
    "        y_pred = lb.transform(y_pred)\n",
    "        return metric(y_test, y_pred, **kwargs)\n",
    "    return score\n",
    "\n",
    "def mk_test(clf, name, mangling=0.0):\n",
    "    def run_test(X, Y, clf=clf, name=name, mangling=mangling):\n",
    "        scoring = {\n",
    "            \"accuracy\":     binarized_scorer(accuracy_score), \n",
    "            \"f1_score\":     binarized_scorer(f1_score, average='macro'), \n",
    "            \"log_loss\":     binarized_scorer(log_loss), \n",
    "            \"precision\":    binarized_scorer(precision_score, average='macro'), \n",
    "            \"recall\":       binarized_scorer(recall_score, average='macro'), \n",
    "            \"roc_auc\":      binarized_scorer(roc_auc_score, average='macro'),\n",
    "            # to je pole pod Precision-Recall, albo jakaś średnia. nie wiem.\n",
    "            \"prec_rec_auc\": binarized_scorer(average_precision_score, average='macro') \n",
    "        }\n",
    "        \n",
    "        '''\n",
    "        # Ta implementacja jest potwornie wolna dla modeli szkolonych na CPU\n",
    "        # ale przynajmniej pozwala na wprowadzenie szumu do danych treningowych\n",
    "        scores = {}\n",
    "        Y_org = np.array(Y, copy=True)\n",
    "        for train_index, test_index in KFold(n_splits=5).split(X):\n",
    "            Y = np.array(Y_org, copy=True)\n",
    "            to_mangle = int(len(train_index)*mangling)\n",
    "            for idx in train_index[:to_mangle]:\n",
    "                Y[idx] = random.choice(classes)\n",
    "            clf.fit(X[train_index], Y[train_index])\n",
    "            Y_true = Y[test_index]\n",
    "            Y_pred = clf.predict(X[test_index])\n",
    "            for (key, scorer) in scoring.items():\n",
    "                if key in scores:\n",
    "                    scores[key] += [scorer(Y_true, Y_pred)]\n",
    "                else:\n",
    "                    scores[key] = [scorer(Y_true, Y_pred)]\n",
    "        '''\n",
    "        \n",
    "        # Bez kross-walidacji.\n",
    "        scores = {}\n",
    "        x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)\n",
    "        to_mangle = int(len(x_train)*mangling)\n",
    "        for idx in range(to_mangle):\n",
    "            y_train[idx] = random.choice(classes)\n",
    "        clf.fit(x_train, y_train)\n",
    "        y_pred = clf.predict(x_test)\n",
    "        for (key, scorer) in scoring.items():\n",
    "            scores[key] = [scorer(y_test, y_pred)]\n",
    "    \n",
    "        scores = {k: (1.0*sum(v))/len(v) for k, v in scores.items()}\n",
    "        df = pd.DataFrame(scores, index=[0])\n",
    "        df.insert(loc=0, column='Name', value=name)\n",
    "        return df\n",
    "    return run_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_adaboost(depth=5, n=100, seed=1):\n",
    "    return AdaBoostClassifier(\n",
    "        base_estimator=DecisionTreeClassifier(max_depth=depth),\n",
    "        n_estimators=n,\n",
    "        random_state=seed)\n",
    "\n",
    "def mk_catboost():\n",
    "    return CatBoostClassifier(iterations=1000, task_type=\"GPU\")\n",
    "\n",
    "class MyLittleKerasClassifier(KerasClassifier):\n",
    "    # predict() nie zwracal \n",
    "    def predict(self, X):\n",
    "        y_pred = KerasClassifier.predict(self, X)\n",
    "        return to_categorical(y_pred, num_classes)\n",
    "\n",
    "def mk_mlp(epochs=10):\n",
    "    def build():\n",
    "        model = Sequential()\n",
    "        model.add(Dense(64, activation='relu', input_shape=(28*28,)))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(128, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(512, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(256, activation='relu'))\n",
    "        model.add(Dropout(0.2))\n",
    "        model.add(Dense(num_classes, activation='softmax'))\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    build().summary()\n",
    "    return MyLittleKerasClassifier(build_fn=build, epochs=epochs)\n",
    "\n",
    "def mk_cnn(epochs=10):\n",
    "    def build():\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(28,28, 1), activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Conv2D(128, kernel_size = 4, activation='relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Flatten())\n",
    "        model.add(Dropout(0.4))\n",
    "        model.add(Dense(10, activation='softmax'))\n",
    "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "    build().summary()\n",
    "    return MyLittleKerasClassifier(build_fn=build, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pomiar metryk w zależności od ilości dostępnych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cnn_data(data_sz):\n",
    "    name = 'CNN | {}% of data'.format(data_sz*100.0)\n",
    "    x, y = mk_dataset(data_sz)\n",
    "    y = to_categorical(y, num_classes)\n",
    "    x /= 255.0\n",
    "    x = x.reshape((x.shape[0], 28, 28, 1))\n",
    "    return mk_test(mk_cnn(epochs=10), name)(x, y)\n",
    "\n",
    "def test_mlp_data(data_sz):\n",
    "    name = 'MLP | {}% of data'.format(data_sz*100.0)\n",
    "    x, y = mk_dataset(data_sz)\n",
    "    y = to_categorical(y, num_classes)\n",
    "    x /= 255.0\n",
    "    return mk_test(mk_mlp(epochs=10), name)(x, y)\n",
    "\n",
    "# Pomiar z użyciem time.time() to nie jest czas CPU\n",
    "# ale już tam nic to. timeit() było zbyt irytujące\n",
    "def test_adaboost_data(data_sz):\n",
    "    print(\"Ada SZ: %d\" % data_sz)\n",
    "    name = 'AdaBoost | {}% of data'.format(data_sz*100.0)\n",
    "    return mk_test(mk_adaboost(n=100), name)(*mk_dataset(data_sz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ada SZ: 0\n",
      "Ada SZ: 0\n",
      "Ada SZ: 0\n",
      "Ada SZ: 0\n",
      "Ada SZ: 0\n",
      "Ada SZ: 1\n",
      "Model: \"sequential_55\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_199 (Dense)            (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_220 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_200 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_221 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_201 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_222 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_202 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_223 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_203 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_224 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_204 (Dense)            (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 357,066\n",
      "Trainable params: 357,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "5600/5600 [==============================] - 2s 353us/step - loss: 0.1865 - accuracy: 0.9246\n",
      "Epoch 2/10\n",
      "5600/5600 [==============================] - 1s 173us/step - loss: 0.1273 - accuracy: 0.9480\n",
      "Epoch 3/10\n",
      "5600/5600 [==============================] - 1s 174us/step - loss: 0.1128 - accuracy: 0.9555\n",
      "Epoch 4/10\n",
      "5600/5600 [==============================] - 1s 171us/step - loss: 0.1073 - accuracy: 0.9563\n",
      "Epoch 5/10\n",
      "5600/5600 [==============================] - 1s 174us/step - loss: 0.1016 - accuracy: 0.9584\n",
      "Epoch 6/10\n",
      "5600/5600 [==============================] - 1s 167us/step - loss: 0.0934 - accuracy: 0.9627\n",
      "Epoch 7/10\n",
      "5600/5600 [==============================] - 1s 179us/step - loss: 0.0905 - accuracy: 0.9640\n",
      "Epoch 8/10\n",
      "5600/5600 [==============================] - 1s 173us/step - loss: 0.0864 - accuracy: 0.9647\n",
      "Epoch 9/10\n",
      "5600/5600 [==============================] - 1s 172us/step - loss: 0.0875 - accuracy: 0.9654\n",
      "Epoch 10/10\n",
      "5600/5600 [==============================] - 1s 172us/step - loss: 0.0876 - accuracy: 0.9658\n",
      "Model: \"sequential_57\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_211 (Dense)            (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_230 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_212 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_231 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_213 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_232 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_214 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_233 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_215 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_234 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_216 (Dense)            (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 357,066\n",
      "Trainable params: 357,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "16800/16800 [==============================] - 4s 236us/step - loss: 0.1397 - accuracy: 0.9439\n",
      "Epoch 2/10\n",
      "16800/16800 [==============================] - 3s 174us/step - loss: 0.1007 - accuracy: 0.9601\n",
      "Epoch 3/10\n",
      "16800/16800 [==============================] - 3s 173us/step - loss: 0.0921 - accuracy: 0.9639\n",
      "Epoch 4/10\n",
      "16800/16800 [==============================] - 3s 172us/step - loss: 0.0857 - accuracy: 0.9663\n",
      "Epoch 5/10\n",
      "16800/16800 [==============================] - 3s 172us/step - loss: 0.0835 - accuracy: 0.9672\n",
      "Epoch 6/10\n",
      "16800/16800 [==============================] - 3s 173us/step - loss: 0.0804 - accuracy: 0.9682\n",
      "Epoch 7/10\n",
      "16800/16800 [==============================] - 3s 174us/step - loss: 0.0770 - accuracy: 0.9696\n",
      "Epoch 8/10\n",
      "16800/16800 [==============================] - 3s 171us/step - loss: 0.0767 - accuracy: 0.9700\n",
      "Epoch 9/10\n",
      "16800/16800 [==============================] - 3s 173us/step - loss: 0.0734 - accuracy: 0.9714\n",
      "Epoch 10/10\n",
      "16800/16800 [==============================] - 3s 173us/step - loss: 0.0739 - accuracy: 0.9713\n",
      "Model: \"sequential_59\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_223 (Dense)            (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_240 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_224 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_241 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_225 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_242 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_226 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_243 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_227 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_244 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_228 (Dense)            (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 357,066\n",
      "Trainable params: 357,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "28000/28000 [==============================] - 6s 214us/step - loss: 0.1256 - accuracy: 0.9494\n",
      "Epoch 2/10\n",
      "28000/28000 [==============================] - 5s 179us/step - loss: 0.0931 - accuracy: 0.9631\n",
      "Epoch 3/10\n",
      "28000/28000 [==============================] - 5s 178us/step - loss: 0.0855 - accuracy: 0.9661\n",
      "Epoch 4/10\n",
      "28000/28000 [==============================] - 5s 183us/step - loss: 0.0810 - accuracy: 0.9678\n",
      "Epoch 5/10\n",
      "28000/28000 [==============================] - 5s 178us/step - loss: 0.0774 - accuracy: 0.9693\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000/28000 [==============================] - 5s 175us/step - loss: 0.0752 - accuracy: 0.9703\n",
      "Epoch 7/10\n",
      "28000/28000 [==============================] - 5s 175us/step - loss: 0.0732 - accuracy: 0.9710\n",
      "Epoch 8/10\n",
      "28000/28000 [==============================] - 5s 175us/step - loss: 0.0723 - accuracy: 0.9715\n",
      "Epoch 9/10\n",
      "28000/28000 [==============================] - 5s 177us/step - loss: 0.0702 - accuracy: 0.9720\n",
      "Epoch 10/10\n",
      "28000/28000 [==============================] - 5s 175us/step - loss: 0.0697 - accuracy: 0.9721\n",
      "Model: \"sequential_61\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_235 (Dense)            (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_250 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_236 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_251 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_237 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_252 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_238 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_253 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_239 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_254 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_240 (Dense)            (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 357,066\n",
      "Trainable params: 357,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "33600/33600 [==============================] - 7s 208us/step - loss: 0.1220 - accuracy: 0.9513\n",
      "Epoch 2/10\n",
      "33600/33600 [==============================] - 6s 178us/step - loss: 0.0904 - accuracy: 0.9648\n",
      "Epoch 3/10\n",
      "33600/33600 [==============================] - 6s 177us/step - loss: 0.0837 - accuracy: 0.9672\n",
      "Epoch 4/10\n",
      "33600/33600 [==============================] - 6s 177us/step - loss: 0.0777 - accuracy: 0.9695\n",
      "Epoch 5/10\n",
      "33600/33600 [==============================] - 6s 179us/step - loss: 0.0764 - accuracy: 0.9704\n",
      "Epoch 6/10\n",
      "33600/33600 [==============================] - 6s 177us/step - loss: 0.0738 - accuracy: 0.9714\n",
      "Epoch 7/10\n",
      "33600/33600 [==============================] - 6s 177us/step - loss: 0.0715 - accuracy: 0.9721\n",
      "Epoch 8/10\n",
      "33600/33600 [==============================] - 6s 177us/step - loss: 0.0711 - accuracy: 0.9725\n",
      "Epoch 9/10\n",
      "33600/33600 [==============================] - 6s 175us/step - loss: 0.0692 - accuracy: 0.9730\n",
      "Epoch 10/10\n",
      "33600/33600 [==============================] - 6s 178us/step - loss: 0.0684 - accuracy: 0.9735\n",
      "Model: \"sequential_63\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_247 (Dense)            (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_260 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_248 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_261 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_249 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_262 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_250 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_263 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_251 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_264 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_252 (Dense)            (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 357,066\n",
      "Trainable params: 357,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "44800/44800 [==============================] - 9s 206us/step - loss: 0.1177 - accuracy: 0.9522\n",
      "Epoch 2/10\n",
      "44800/44800 [==============================] - 8s 176us/step - loss: 0.0884 - accuracy: 0.9654\n",
      "Epoch 3/10\n",
      "44800/44800 [==============================] - 8s 177us/step - loss: 0.0826 - accuracy: 0.9679\n",
      "Epoch 4/10\n",
      "44800/44800 [==============================] - 8s 183us/step - loss: 0.0786 - accuracy: 0.9695\n",
      "Epoch 5/10\n",
      "44800/44800 [==============================] - 8s 183us/step - loss: 0.0749 - accuracy: 0.9710\n",
      "Epoch 6/10\n",
      "44800/44800 [==============================] - 8s 183us/step - loss: 0.0734 - accuracy: 0.9715\n",
      "Epoch 7/10\n",
      "44800/44800 [==============================] - 8s 185us/step - loss: 0.0712 - accuracy: 0.9722\n",
      "Epoch 8/10\n",
      "44800/44800 [==============================] - 8s 184us/step - loss: 0.0695 - accuracy: 0.9731\n",
      "Epoch 9/10\n",
      "44800/44800 [==============================] - 8s 188us/step - loss: 0.0691 - accuracy: 0.9732\n",
      "Epoch 10/10\n",
      "44800/44800 [==============================] - 8s 187us/step - loss: 0.0669 - accuracy: 0.9741\n",
      "Model: \"sequential_65\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_259 (Dense)            (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_270 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_260 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_271 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_261 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_272 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_262 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_273 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_263 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_274 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_264 (Dense)            (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 357,066\n",
      "Trainable params: 357,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "56000/56000 [==============================] - 12s 210us/step - loss: 0.1115 - accuracy: 0.9555\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56000/56000 [==============================] - 11s 189us/step - loss: 0.0858 - accuracy: 0.9665\n",
      "Epoch 3/10\n",
      "56000/56000 [==============================] - 11s 188us/step - loss: 0.0789 - accuracy: 0.9690\n",
      "Epoch 4/10\n",
      "56000/56000 [==============================] - 11s 188us/step - loss: 0.0755 - accuracy: 0.9706\n",
      "Epoch 5/10\n",
      "56000/56000 [==============================] - 11s 189us/step - loss: 0.0731 - accuracy: 0.9714\n",
      "Epoch 6/10\n",
      "56000/56000 [==============================] - 10s 187us/step - loss: 0.0706 - accuracy: 0.9723\n",
      "Epoch 7/10\n",
      "56000/56000 [==============================] - 11s 189us/step - loss: 0.0690 - accuracy: 0.9727\n",
      "Epoch 8/10\n",
      "56000/56000 [==============================] - 11s 189us/step - loss: 0.0680 - accuracy: 0.9734\n",
      "Epoch 9/10\n",
      "56000/56000 [==============================] - 11s 190us/step - loss: 0.0666 - accuracy: 0.9739\n",
      "Epoch 10/10\n",
      "56000/56000 [==============================] - 11s 191us/step - loss: 0.0649 - accuracy: 0.9745\n",
      "Model: \"sequential_67\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_176 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_176 (Bat (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_177 (Conv2D)          (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_177 (Bat (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_178 (Conv2D)          (None, 12, 12, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_178 (Bat (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_280 (Dropout)        (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_179 (Conv2D)          (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_179 (Bat (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_180 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_180 (Bat (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_181 (Conv2D)          (None, 4, 4, 64)          102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_181 (Bat (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_281 (Dropout)        (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_182 (Conv2D)          (None, 1, 1, 128)         131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_182 (Bat (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_26 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_282 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_271 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 327,242\n",
      "Trainable params: 326,410\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "5600/5600 [==============================] - 4s 627us/step - loss: 0.1779 - accuracy: 0.9336\n",
      "Epoch 2/10\n",
      "5600/5600 [==============================] - 2s 349us/step - loss: 0.1174 - accuracy: 0.9527\n",
      "Epoch 3/10\n",
      "5600/5600 [==============================] - 2s 349us/step - loss: 0.0991 - accuracy: 0.9583\n",
      "Epoch 4/10\n",
      "5600/5600 [==============================] - 2s 350us/step - loss: 0.0877 - accuracy: 0.9651\n",
      "Epoch 5/10\n",
      "5600/5600 [==============================] - 2s 356us/step - loss: 0.0817 - accuracy: 0.9682\n",
      "Epoch 6/10\n",
      "5600/5600 [==============================] - 2s 352us/step - loss: 0.0740 - accuracy: 0.9700\n",
      "Epoch 7/10\n",
      "5600/5600 [==============================] - 2s 350us/step - loss: 0.0703 - accuracy: 0.9722\n",
      "Epoch 8/10\n",
      "5600/5600 [==============================] - 2s 350us/step - loss: 0.0667 - accuracy: 0.9737\n",
      "Epoch 9/10\n",
      "5600/5600 [==============================] - 2s 349us/step - loss: 0.0612 - accuracy: 0.9754\n",
      "Epoch 10/10\n",
      "5600/5600 [==============================] - 2s 349us/step - loss: 0.0592 - accuracy: 0.9765\n",
      "Model: \"sequential_69\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_190 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_190 (Bat (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_191 (Conv2D)          (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_191 (Bat (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_192 (Conv2D)          (None, 12, 12, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_192 (Bat (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_286 (Dropout)        (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_193 (Conv2D)          (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_193 (Bat (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_194 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_194 (Bat (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_195 (Conv2D)          (None, 4, 4, 64)          102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_195 (Bat (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_287 (Dropout)        (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_196 (Conv2D)          (None, 1, 1, 128)         131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_196 (Bat (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_28 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_288 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_273 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 327,242\n",
      "Trainable params: 326,410\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "16800/16800 [==============================] - 8s 459us/step - loss: 0.1329 - accuracy: 0.9481\n",
      "Epoch 2/10\n",
      "16800/16800 [==============================] - 6s 357us/step - loss: 0.0876 - accuracy: 0.9652\n",
      "Epoch 3/10\n",
      "16800/16800 [==============================] - 6s 356us/step - loss: 0.0748 - accuracy: 0.9705\n",
      "Epoch 4/10\n",
      "16800/16800 [==============================] - 6s 377us/step - loss: 0.0666 - accuracy: 0.9739\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16800/16800 [==============================] - 6s 370us/step - loss: 0.0620 - accuracy: 0.9760\n",
      "Epoch 6/10\n",
      "16800/16800 [==============================] - 6s 372us/step - loss: 0.0572 - accuracy: 0.9777\n",
      "Epoch 7/10\n",
      "16800/16800 [==============================] - 6s 360us/step - loss: 0.0541 - accuracy: 0.9787\n",
      "Epoch 8/10\n",
      "16800/16800 [==============================] - 6s 359us/step - loss: 0.0515 - accuracy: 0.9795\n",
      "Epoch 9/10\n",
      "16800/16800 [==============================] - 6s 366us/step - loss: 0.0498 - accuracy: 0.9804\n",
      "Epoch 10/10\n",
      "16800/16800 [==============================] - 6s 353us/step - loss: 0.0471 - accuracy: 0.9814\n",
      "Model: \"sequential_71\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_204 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_204 (Bat (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_205 (Conv2D)          (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_205 (Bat (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_206 (Conv2D)          (None, 12, 12, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_206 (Bat (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_292 (Dropout)        (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_207 (Conv2D)          (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_207 (Bat (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_208 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_208 (Bat (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_209 (Conv2D)          (None, 4, 4, 64)          102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_209 (Bat (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_293 (Dropout)        (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_210 (Conv2D)          (None, 1, 1, 128)         131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_210 (Bat (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_30 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_294 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_275 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 327,242\n",
      "Trainable params: 326,410\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "28000/28000 [==============================] - 12s 422us/step - loss: 0.1163 - accuracy: 0.9539\n",
      "Epoch 2/10\n",
      "28000/28000 [==============================] - 10s 361us/step - loss: 0.0764 - accuracy: 0.9698\n",
      "Epoch 3/10\n",
      "28000/28000 [==============================] - 10s 362us/step - loss: 0.0655 - accuracy: 0.9738\n",
      "Epoch 4/10\n",
      "28000/28000 [==============================] - 10s 360us/step - loss: 0.0599 - accuracy: 0.9762\n",
      "Epoch 5/10\n",
      "28000/28000 [==============================] - 10s 361us/step - loss: 0.0557 - accuracy: 0.9782\n",
      "Epoch 6/10\n",
      "28000/28000 [==============================] - 10s 362us/step - loss: 0.0508 - accuracy: 0.9802\n",
      "Epoch 7/10\n",
      "28000/28000 [==============================] - 10s 359us/step - loss: 0.0481 - accuracy: 0.9814\n",
      "Epoch 8/10\n",
      "28000/28000 [==============================] - 10s 363us/step - loss: 0.0462 - accuracy: 0.9820\n",
      "Epoch 9/10\n",
      "28000/28000 [==============================] - 10s 362us/step - loss: 0.0432 - accuracy: 0.9835\n",
      "Epoch 10/10\n",
      "28000/28000 [==============================] - 10s 360us/step - loss: 0.0403 - accuracy: 0.9844\n",
      "Model: \"sequential_73\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_218 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_218 (Bat (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_219 (Conv2D)          (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_219 (Bat (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_220 (Conv2D)          (None, 12, 12, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_220 (Bat (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_298 (Dropout)        (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_221 (Conv2D)          (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_221 (Bat (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_222 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_222 (Bat (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_223 (Conv2D)          (None, 4, 4, 64)          102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_223 (Bat (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_299 (Dropout)        (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_224 (Conv2D)          (None, 1, 1, 128)         131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_224 (Bat (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_32 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_300 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_277 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 327,242\n",
      "Trainable params: 326,410\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "33600/33600 [==============================] - 14s 417us/step - loss: 0.1096 - accuracy: 0.9571\n",
      "Epoch 2/10\n",
      "33600/33600 [==============================] - 12s 367us/step - loss: 0.0724 - accuracy: 0.9713\n",
      "Epoch 3/10\n",
      "33600/33600 [==============================] - 13s 383us/step - loss: 0.0622 - accuracy: 0.9756\n",
      "Epoch 4/10\n",
      "33600/33600 [==============================] - 12s 368us/step - loss: 0.0553 - accuracy: 0.9784\n",
      "Epoch 5/10\n",
      "33600/33600 [==============================] - 12s 366us/step - loss: 0.0508 - accuracy: 0.9803\n",
      "Epoch 6/10\n",
      "33600/33600 [==============================] - 12s 365us/step - loss: 0.0485 - accuracy: 0.9811\n",
      "Epoch 7/10\n",
      "33600/33600 [==============================] - 12s 367us/step - loss: 0.0450 - accuracy: 0.9824\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33600/33600 [==============================] - 12s 364us/step - loss: 0.0420 - accuracy: 0.9837\n",
      "Epoch 9/10\n",
      "33600/33600 [==============================] - 12s 364us/step - loss: 0.0396 - accuracy: 0.9848\n",
      "Epoch 10/10\n",
      "33600/33600 [==============================] - 12s 368us/step - loss: 0.0371 - accuracy: 0.9857\n",
      "Model: \"sequential_75\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_232 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_232 (Bat (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_233 (Conv2D)          (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_233 (Bat (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_234 (Conv2D)          (None, 12, 12, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_234 (Bat (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_304 (Dropout)        (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_235 (Conv2D)          (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_235 (Bat (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_236 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_236 (Bat (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_237 (Conv2D)          (None, 4, 4, 64)          102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_237 (Bat (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_305 (Dropout)        (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_238 (Conv2D)          (None, 1, 1, 128)         131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_238 (Bat (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_34 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_306 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_279 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 327,242\n",
      "Trainable params: 326,410\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "44800/44800 [==============================] - 20s 445us/step - loss: 0.1026 - accuracy: 0.9597\n",
      "Epoch 2/10\n",
      "44800/44800 [==============================] - 17s 385us/step - loss: 0.0655 - accuracy: 0.9746\n",
      "Epoch 3/10\n",
      "44800/44800 [==============================] - 17s 370us/step - loss: 0.0561 - accuracy: 0.9784\n",
      "Epoch 4/10\n",
      "44800/44800 [==============================] - 16s 359us/step - loss: 0.0512 - accuracy: 0.9803\n",
      "Epoch 5/10\n",
      "44800/44800 [==============================] - 17s 381us/step - loss: 0.0473 - accuracy: 0.9819\n",
      "Epoch 6/10\n",
      "44800/44800 [==============================] - 17s 377us/step - loss: 0.0439 - accuracy: 0.9831\n",
      "Epoch 7/10\n",
      "44800/44800 [==============================] - 17s 376us/step - loss: 0.0407 - accuracy: 0.9845\n",
      "Epoch 8/10\n",
      "44800/44800 [==============================] - 17s 383us/step - loss: 0.0387 - accuracy: 0.9851\n",
      "Epoch 9/10\n",
      "44800/44800 [==============================] - 17s 377us/step - loss: 0.0358 - accuracy: 0.9864\n",
      "Epoch 10/10\n",
      "44800/44800 [==============================] - 17s 379us/step - loss: 0.0334 - accuracy: 0.9876\n",
      "Model: \"sequential_77\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_246 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_246 (Bat (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_247 (Conv2D)          (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_247 (Bat (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_248 (Conv2D)          (None, 12, 12, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_248 (Bat (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_310 (Dropout)        (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_249 (Conv2D)          (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_249 (Bat (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_250 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_250 (Bat (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_251 (Conv2D)          (None, 4, 4, 64)          102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_251 (Bat (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_311 (Dropout)        (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_252 (Conv2D)          (None, 1, 1, 128)         131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_252 (Bat (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_36 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_312 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_281 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 327,242\n",
      "Trainable params: 326,410\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "56000/56000 [==============================] - 24s 424us/step - loss: 0.0946 - accuracy: 0.9627\n",
      "Epoch 2/10\n",
      "56000/56000 [==============================] - 22s 385us/step - loss: 0.0617 - accuracy: 0.9761\n",
      "Epoch 3/10\n",
      "56000/56000 [==============================] - 22s 385us/step - loss: 0.0538 - accuracy: 0.9790\n",
      "Epoch 4/10\n",
      "56000/56000 [==============================] - 22s 393us/step - loss: 0.0492 - accuracy: 0.9809\n",
      "Epoch 5/10\n",
      "56000/56000 [==============================] - 23s 413us/step - loss: 0.0445 - accuracy: 0.9829\n",
      "Epoch 6/10\n",
      "56000/56000 [==============================] - 23s 402us/step - loss: 0.0413 - accuracy: 0.9843\n",
      "Epoch 7/10\n",
      "56000/56000 [==============================] - 21s 383us/step - loss: 0.0389 - accuracy: 0.9850\n",
      "Epoch 8/10\n",
      "56000/56000 [==============================] - 23s 402us/step - loss: 0.0365 - accuracy: 0.9863\n",
      "Epoch 9/10\n",
      "56000/56000 [==============================] - 22s 402us/step - loss: 0.0337 - accuracy: 0.9872\n",
      "Epoch 10/10\n",
      "56000/56000 [==============================] - 24s 425us/step - loss: 0.0319 - accuracy: 0.9878\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>log_loss</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>prec_rec_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost | 10.0% of data</td>\n",
       "      <td>0.739286</td>\n",
       "      <td>0.748010</td>\n",
       "      <td>9.004752</td>\n",
       "      <td>0.753317</td>\n",
       "      <td>0.747010</td>\n",
       "      <td>0.858962</td>\n",
       "      <td>0.618273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost | 30.0% of data</td>\n",
       "      <td>0.746190</td>\n",
       "      <td>0.748015</td>\n",
       "      <td>8.766270</td>\n",
       "      <td>0.752440</td>\n",
       "      <td>0.745637</td>\n",
       "      <td>0.858730</td>\n",
       "      <td>0.624200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost | 50.0% of data</td>\n",
       "      <td>0.710286</td>\n",
       "      <td>0.718291</td>\n",
       "      <td>10.006377</td>\n",
       "      <td>0.728679</td>\n",
       "      <td>0.715963</td>\n",
       "      <td>0.841832</td>\n",
       "      <td>0.583490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost | 60.0% of data</td>\n",
       "      <td>0.716548</td>\n",
       "      <td>0.719405</td>\n",
       "      <td>9.790098</td>\n",
       "      <td>0.732555</td>\n",
       "      <td>0.717996</td>\n",
       "      <td>0.843254</td>\n",
       "      <td>0.586704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost | 80.0% of data</td>\n",
       "      <td>0.737321</td>\n",
       "      <td>0.741090</td>\n",
       "      <td>9.072596</td>\n",
       "      <td>0.745271</td>\n",
       "      <td>0.739609</td>\n",
       "      <td>0.855196</td>\n",
       "      <td>0.612381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost | 100.0% of data</td>\n",
       "      <td>0.690214</td>\n",
       "      <td>0.693554</td>\n",
       "      <td>10.699620</td>\n",
       "      <td>0.709862</td>\n",
       "      <td>0.690326</td>\n",
       "      <td>0.827950</td>\n",
       "      <td>0.552017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP | 10.0% of data</td>\n",
       "      <td>0.845000</td>\n",
       "      <td>0.841632</td>\n",
       "      <td>5.353510</td>\n",
       "      <td>0.843769</td>\n",
       "      <td>0.845330</td>\n",
       "      <td>0.914048</td>\n",
       "      <td>0.742177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP | 30.0% of data</td>\n",
       "      <td>0.860952</td>\n",
       "      <td>0.860672</td>\n",
       "      <td>4.802535</td>\n",
       "      <td>0.862481</td>\n",
       "      <td>0.860372</td>\n",
       "      <td>0.922443</td>\n",
       "      <td>0.769402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP | 50.0% of data</td>\n",
       "      <td>0.867571</td>\n",
       "      <td>0.864721</td>\n",
       "      <td>4.573921</td>\n",
       "      <td>0.867263</td>\n",
       "      <td>0.867241</td>\n",
       "      <td>0.926259</td>\n",
       "      <td>0.773778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP | 60.0% of data</td>\n",
       "      <td>0.871071</td>\n",
       "      <td>0.869019</td>\n",
       "      <td>4.453035</td>\n",
       "      <td>0.873707</td>\n",
       "      <td>0.870241</td>\n",
       "      <td>0.927957</td>\n",
       "      <td>0.782170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP | 80.0% of data</td>\n",
       "      <td>0.865625</td>\n",
       "      <td>0.862953</td>\n",
       "      <td>4.641148</td>\n",
       "      <td>0.871445</td>\n",
       "      <td>0.865380</td>\n",
       "      <td>0.925225</td>\n",
       "      <td>0.775477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP | 100.0% of data</td>\n",
       "      <td>0.879000</td>\n",
       "      <td>0.877812</td>\n",
       "      <td>4.179192</td>\n",
       "      <td>0.879399</td>\n",
       "      <td>0.879584</td>\n",
       "      <td>0.933066</td>\n",
       "      <td>0.794368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN | 10.0% of data</td>\n",
       "      <td>0.879286</td>\n",
       "      <td>0.876660</td>\n",
       "      <td>4.169324</td>\n",
       "      <td>0.880998</td>\n",
       "      <td>0.878928</td>\n",
       "      <td>0.932770</td>\n",
       "      <td>0.794215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN | 30.0% of data</td>\n",
       "      <td>0.896667</td>\n",
       "      <td>0.897043</td>\n",
       "      <td>3.569007</td>\n",
       "      <td>0.898017</td>\n",
       "      <td>0.897392</td>\n",
       "      <td>0.942946</td>\n",
       "      <td>0.823279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN | 50.0% of data</td>\n",
       "      <td>0.916571</td>\n",
       "      <td>0.914474</td>\n",
       "      <td>2.881521</td>\n",
       "      <td>0.916018</td>\n",
       "      <td>0.915040</td>\n",
       "      <td>0.952888</td>\n",
       "      <td>0.850494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN | 60.0% of data</td>\n",
       "      <td>0.917738</td>\n",
       "      <td>0.918797</td>\n",
       "      <td>2.841226</td>\n",
       "      <td>0.921193</td>\n",
       "      <td>0.918597</td>\n",
       "      <td>0.954728</td>\n",
       "      <td>0.857736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN | 80.0% of data</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>0.937263</td>\n",
       "      <td>2.158674</td>\n",
       "      <td>0.937769</td>\n",
       "      <td>0.937783</td>\n",
       "      <td>0.965419</td>\n",
       "      <td>0.887928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN | 100.0% of data</td>\n",
       "      <td>0.938643</td>\n",
       "      <td>0.938969</td>\n",
       "      <td>2.119201</td>\n",
       "      <td>0.939983</td>\n",
       "      <td>0.939001</td>\n",
       "      <td>0.966094</td>\n",
       "      <td>0.891338</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        Name  accuracy  f1_score   log_loss  precision  \\\n",
       "0   AdaBoost | 10.0% of data  0.739286  0.748010   9.004752   0.753317   \n",
       "0   AdaBoost | 30.0% of data  0.746190  0.748015   8.766270   0.752440   \n",
       "0   AdaBoost | 50.0% of data  0.710286  0.718291  10.006377   0.728679   \n",
       "0   AdaBoost | 60.0% of data  0.716548  0.719405   9.790098   0.732555   \n",
       "0   AdaBoost | 80.0% of data  0.737321  0.741090   9.072596   0.745271   \n",
       "0  AdaBoost | 100.0% of data  0.690214  0.693554  10.699620   0.709862   \n",
       "0        MLP | 10.0% of data  0.845000  0.841632   5.353510   0.843769   \n",
       "0        MLP | 30.0% of data  0.860952  0.860672   4.802535   0.862481   \n",
       "0        MLP | 50.0% of data  0.867571  0.864721   4.573921   0.867263   \n",
       "0        MLP | 60.0% of data  0.871071  0.869019   4.453035   0.873707   \n",
       "0        MLP | 80.0% of data  0.865625  0.862953   4.641148   0.871445   \n",
       "0       MLP | 100.0% of data  0.879000  0.877812   4.179192   0.879399   \n",
       "0        CNN | 10.0% of data  0.879286  0.876660   4.169324   0.880998   \n",
       "0        CNN | 30.0% of data  0.896667  0.897043   3.569007   0.898017   \n",
       "0        CNN | 50.0% of data  0.916571  0.914474   2.881521   0.916018   \n",
       "0        CNN | 60.0% of data  0.917738  0.918797   2.841226   0.921193   \n",
       "0        CNN | 80.0% of data  0.937500  0.937263   2.158674   0.937769   \n",
       "0       CNN | 100.0% of data  0.938643  0.938969   2.119201   0.939983   \n",
       "\n",
       "     recall   roc_auc  prec_rec_auc  \n",
       "0  0.747010  0.858962      0.618273  \n",
       "0  0.745637  0.858730      0.624200  \n",
       "0  0.715963  0.841832      0.583490  \n",
       "0  0.717996  0.843254      0.586704  \n",
       "0  0.739609  0.855196      0.612381  \n",
       "0  0.690326  0.827950      0.552017  \n",
       "0  0.845330  0.914048      0.742177  \n",
       "0  0.860372  0.922443      0.769402  \n",
       "0  0.867241  0.926259      0.773778  \n",
       "0  0.870241  0.927957      0.782170  \n",
       "0  0.865380  0.925225      0.775477  \n",
       "0  0.879584  0.933066      0.794368  \n",
       "0  0.878928  0.932770      0.794215  \n",
       "0  0.897392  0.942946      0.823279  \n",
       "0  0.915040  0.952888      0.850494  \n",
       "0  0.918597  0.954728      0.857736  \n",
       "0  0.937783  0.965419      0.887928  \n",
       "0  0.939001  0.966094      0.891338  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests_data = pd.concat([method(data_sz) for method in [test_adaboost_data, test_mlp_data, test_cnn_data] for data_sz in [0.1, 0.3, 0.5, 0.6, 0.8, 1.0]])\n",
    "tests_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tests_data.to_pickle('3aa.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pomiar metryk w zależności od czasów treningu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cnn_time(epochs, data_sz=0.5):\n",
    "    name = 'CNN | %d epochs' % epochs\n",
    "    x, y = mk_dataset(data_sz)\n",
    "    y = to_categorical(y, num_classes)\n",
    "    x /= 255.0\n",
    "    x = x.reshape((x.shape[0], 28, 28, 1))\n",
    "    return mk_test(mk_cnn(epochs=epochs), name)(x, y)\n",
    "\n",
    "def test_mlp_time(epochs, data_sz=0.5):\n",
    "    name = 'MLP | %d epochs' % epochs\n",
    "    x, y = mk_dataset(data_sz)\n",
    "    y = to_categorical(y, num_classes)\n",
    "    x /= 255.0\n",
    "    return mk_test(mk_mlp(epochs=epochs), name)(x, y)\n",
    "\n",
    "# Pomiar z użyciem time.time() to nie jest czas CPU\n",
    "# ale już tam nic to. timeit() było zbyt irytujące\n",
    "def test_adaboost_time(epochs, data_sz=0.5):\n",
    "    print(\"AdaBoost %d\" % epochs)\n",
    "    start_time = time.time()\n",
    "    name = 'AdaBoost'\n",
    "    result = mk_test(mk_adaboost(n=epochs), name)(*mk_dataset(data_sz))\n",
    "    elapsed_time = time.time() - start_time\n",
    "    result['Name'] += ' | %d secs' % (elapsed_time)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dla Adaboosta - czas rzeczywisty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost 10\n",
      "AdaBoost 25\n",
      "AdaBoost 50\n",
      "AdaBoost 150\n",
      "AdaBoost 500\n"
     ]
    }
   ],
   "source": [
    "tests_time = [test_adaboost_time(n) for n in [10, 25, 50, 150, 500]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dla sieci - liczba epok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_79\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_283 (Dense)            (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_316 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_284 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_317 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_285 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_318 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_286 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_319 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_287 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_320 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_288 (Dense)            (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 357,066\n",
      "Trainable params: 357,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "28000/28000 [==============================] - 8s 284us/step - loss: 0.1245 - accuracy: 0.9504\n",
      "Model: \"sequential_81\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_295 (Dense)            (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_326 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_296 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_327 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_297 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_328 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_298 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_329 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_299 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_330 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_300 (Dense)            (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 357,066\n",
      "Trainable params: 357,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/3\n",
      "28000/28000 [==============================] - 8s 292us/step - loss: 0.1258 - accuracy: 0.9494\n",
      "Epoch 2/3\n",
      "28000/28000 [==============================] - 6s 227us/step - loss: 0.0922 - accuracy: 0.9638\n",
      "Epoch 3/3\n",
      "28000/28000 [==============================] - 6s 231us/step - loss: 0.0839 - accuracy: 0.9674\n",
      "Model: \"sequential_83\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_307 (Dense)            (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_336 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_308 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_337 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_309 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_338 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_310 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_339 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_311 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_340 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_312 (Dense)            (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 357,066\n",
      "Trainable params: 357,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "28000/28000 [==============================] - 8s 282us/step - loss: 0.1265 - accuracy: 0.9491\n",
      "Epoch 2/5\n",
      "28000/28000 [==============================] - 6s 229us/step - loss: 0.0919 - accuracy: 0.9634\n",
      "Epoch 3/5\n",
      "28000/28000 [==============================] - 6s 227us/step - loss: 0.0844 - accuracy: 0.9663\n",
      "Epoch 4/5\n",
      "28000/28000 [==============================] - 6s 229us/step - loss: 0.0810 - accuracy: 0.9679\n",
      "Epoch 5/5\n",
      "28000/28000 [==============================] - 6s 232us/step - loss: 0.0777 - accuracy: 0.9689\n",
      "Model: \"sequential_85\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_319 (Dense)            (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_346 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_320 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_347 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_321 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_348 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_322 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_349 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_323 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_350 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_324 (Dense)            (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 357,066\n",
      "Trainable params: 357,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000/28000 [==============================] - 8s 284us/step - loss: 0.1267 - accuracy: 0.9488\n",
      "Epoch 2/15\n",
      "28000/28000 [==============================] - 7s 233us/step - loss: 0.0923 - accuracy: 0.9638\n",
      "Epoch 3/15\n",
      "28000/28000 [==============================] - 6s 230us/step - loss: 0.0858 - accuracy: 0.9661\n",
      "Epoch 4/15\n",
      "28000/28000 [==============================] - 7s 232us/step - loss: 0.0800 - accuracy: 0.9689\n",
      "Epoch 5/15\n",
      "28000/28000 [==============================] - 6s 229us/step - loss: 0.0776 - accuracy: 0.9697\n",
      "Epoch 6/15\n",
      "28000/28000 [==============================] - 6s 231us/step - loss: 0.0764 - accuracy: 0.9702\n",
      "Epoch 7/15\n",
      "28000/28000 [==============================] - 6s 230us/step - loss: 0.0726 - accuracy: 0.9715\n",
      "Epoch 8/15\n",
      "28000/28000 [==============================] - 6s 229us/step - loss: 0.0700 - accuracy: 0.9728\n",
      "Epoch 9/15\n",
      "28000/28000 [==============================] - 7s 233us/step - loss: 0.0701 - accuracy: 0.9726\n",
      "Epoch 10/15\n",
      "28000/28000 [==============================] - 6s 229us/step - loss: 0.0677 - accuracy: 0.9735\n",
      "Epoch 11/15\n",
      "28000/28000 [==============================] - 6s 229us/step - loss: 0.0671 - accuracy: 0.9739\n",
      "Epoch 12/15\n",
      "28000/28000 [==============================] - 7s 233us/step - loss: 0.0674 - accuracy: 0.9741\n",
      "Epoch 13/15\n",
      "28000/28000 [==============================] - 6s 230us/step - loss: 0.0661 - accuracy: 0.9742\n",
      "Epoch 14/15\n",
      "28000/28000 [==============================] - 7s 232us/step - loss: 0.0648 - accuracy: 0.9752\n",
      "Epoch 15/15\n",
      "28000/28000 [==============================] - 6s 230us/step - loss: 0.0642 - accuracy: 0.9752\n",
      "Model: \"sequential_87\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_331 (Dense)            (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_356 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_332 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_357 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_333 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_358 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_334 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_359 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_335 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_360 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_336 (Dense)            (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 357,066\n",
      "Trainable params: 357,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "28000/28000 [==============================] - 9s 308us/step - loss: 0.1277 - accuracy: 0.9478\n",
      "Epoch 2/25\n",
      "28000/28000 [==============================] - 7s 234us/step - loss: 0.0946 - accuracy: 0.9625\n",
      "Epoch 3/25\n",
      "28000/28000 [==============================] - 7s 233us/step - loss: 0.0862 - accuracy: 0.9660\n",
      "Epoch 4/25\n",
      "28000/28000 [==============================] - 7s 236us/step - loss: 0.0826 - accuracy: 0.9679\n",
      "Epoch 5/25\n",
      "28000/28000 [==============================] - 7s 233us/step - loss: 0.0786 - accuracy: 0.9691\n",
      "Epoch 6/25\n",
      "28000/28000 [==============================] - 7s 238us/step - loss: 0.0766 - accuracy: 0.9702\n",
      "Epoch 7/25\n",
      "28000/28000 [==============================] - 7s 236us/step - loss: 0.0753 - accuracy: 0.9706\n",
      "Epoch 8/25\n",
      "28000/28000 [==============================] - 7s 237us/step - loss: 0.0728 - accuracy: 0.9716\n",
      "Epoch 9/25\n",
      "28000/28000 [==============================] - 7s 241us/step - loss: 0.0715 - accuracy: 0.9722\n",
      "Epoch 10/25\n",
      "28000/28000 [==============================] - 7s 240us/step - loss: 0.0694 - accuracy: 0.9729\n",
      "Epoch 11/25\n",
      "28000/28000 [==============================] - 7s 237us/step - loss: 0.0689 - accuracy: 0.9731\n",
      "Epoch 12/25\n",
      "28000/28000 [==============================] - 7s 236us/step - loss: 0.0687 - accuracy: 0.9733\n",
      "Epoch 13/25\n",
      "28000/28000 [==============================] - 7s 237us/step - loss: 0.0668 - accuracy: 0.9738\n",
      "Epoch 14/25\n",
      "28000/28000 [==============================] - 7s 234us/step - loss: 0.0656 - accuracy: 0.9744\n",
      "Epoch 15/25\n",
      "28000/28000 [==============================] - 7s 235us/step - loss: 0.0660 - accuracy: 0.9742\n",
      "Epoch 16/25\n",
      "28000/28000 [==============================] - 7s 237us/step - loss: 0.0645 - accuracy: 0.9747\n",
      "Epoch 17/25\n",
      "28000/28000 [==============================] - 7s 233us/step - loss: 0.0631 - accuracy: 0.9754\n",
      "Epoch 18/25\n",
      "28000/28000 [==============================] - 7s 237us/step - loss: 0.0635 - accuracy: 0.9753\n",
      "Epoch 19/25\n",
      "28000/28000 [==============================] - 7s 234us/step - loss: 0.0632 - accuracy: 0.9754\n",
      "Epoch 20/25\n",
      "28000/28000 [==============================] - 7s 234us/step - loss: 0.0627 - accuracy: 0.9758\n",
      "Epoch 21/25\n",
      "28000/28000 [==============================] - 7s 237us/step - loss: 0.0617 - accuracy: 0.9758\n",
      "Epoch 22/25\n",
      "28000/28000 [==============================] - 7s 234us/step - loss: 0.0614 - accuracy: 0.9764\n",
      "Epoch 23/25\n",
      "28000/28000 [==============================] - 7s 237us/step - loss: 0.0606 - accuracy: 0.9767\n",
      "Epoch 24/25\n",
      "28000/28000 [==============================] - 7s 237us/step - loss: 0.0613 - accuracy: 0.9766\n",
      "Epoch 25/25\n",
      "28000/28000 [==============================] - 7s 240us/step - loss: 0.0592 - accuracy: 0.9769\n",
      "Model: \"sequential_89\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_343 (Dense)            (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_366 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_344 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_367 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_345 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_368 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_346 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_369 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_347 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_370 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_348 (Dense)            (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 357,066\n",
      "Trainable params: 357,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/35\n",
      "28000/28000 [==============================] - 8s 294us/step - loss: 0.1286 - accuracy: 0.9485\n",
      "Epoch 2/35\n",
      "28000/28000 [==============================] - 7s 239us/step - loss: 0.0948 - accuracy: 0.9629\n",
      "Epoch 3/35\n",
      "28000/28000 [==============================] - 7s 240us/step - loss: 0.0872 - accuracy: 0.9656\n",
      "Epoch 4/35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000/28000 [==============================] - 7s 235us/step - loss: 0.0825 - accuracy: 0.9680\n",
      "Epoch 5/35\n",
      "28000/28000 [==============================] - 7s 239us/step - loss: 0.0792 - accuracy: 0.9690\n",
      "Epoch 6/35\n",
      "28000/28000 [==============================] - 7s 234us/step - loss: 0.0761 - accuracy: 0.9699\n",
      "Epoch 7/35\n",
      "28000/28000 [==============================] - 7s 237us/step - loss: 0.0754 - accuracy: 0.9705\n",
      "Epoch 8/35\n",
      "28000/28000 [==============================] - 7s 236us/step - loss: 0.0722 - accuracy: 0.9714\n",
      "Epoch 9/35\n",
      "28000/28000 [==============================] - 7s 235us/step - loss: 0.0703 - accuracy: 0.9730\n",
      "Epoch 10/35\n",
      "28000/28000 [==============================] - 7s 254us/step - loss: 0.0707 - accuracy: 0.9726\n",
      "Epoch 11/35\n",
      "28000/28000 [==============================] - 7s 252us/step - loss: 0.0688 - accuracy: 0.9733\n",
      "Epoch 12/35\n",
      "28000/28000 [==============================] - 7s 255us/step - loss: 0.0674 - accuracy: 0.9736\n",
      "Epoch 13/35\n",
      "28000/28000 [==============================] - 7s 252us/step - loss: 0.0663 - accuracy: 0.9743\n",
      "Epoch 14/35\n",
      "28000/28000 [==============================] - 7s 254us/step - loss: 0.0660 - accuracy: 0.9743\n",
      "Epoch 15/35\n",
      "28000/28000 [==============================] - 7s 257us/step - loss: 0.0645 - accuracy: 0.9753\n",
      "Epoch 16/35\n",
      "28000/28000 [==============================] - 7s 257us/step - loss: 0.0650 - accuracy: 0.9748\n",
      "Epoch 17/35\n",
      "28000/28000 [==============================] - 7s 256us/step - loss: 0.0637 - accuracy: 0.9755\n",
      "Epoch 18/35\n",
      "28000/28000 [==============================] - 7s 255us/step - loss: 0.0629 - accuracy: 0.9755\n",
      "Epoch 19/35\n",
      "28000/28000 [==============================] - 7s 255us/step - loss: 0.0629 - accuracy: 0.9758\n",
      "Epoch 20/35\n",
      "28000/28000 [==============================] - 7s 251us/step - loss: 0.0619 - accuracy: 0.9758\n",
      "Epoch 21/35\n",
      "28000/28000 [==============================] - 7s 253us/step - loss: 0.0605 - accuracy: 0.9767\n",
      "Epoch 22/35\n",
      "28000/28000 [==============================] - 7s 250us/step - loss: 0.0601 - accuracy: 0.9763\n",
      "Epoch 23/35\n",
      "28000/28000 [==============================] - 7s 252us/step - loss: 0.0607 - accuracy: 0.9761\n",
      "Epoch 24/35\n",
      "28000/28000 [==============================] - 7s 253us/step - loss: 0.0596 - accuracy: 0.9766\n",
      "Epoch 25/35\n",
      "28000/28000 [==============================] - 7s 256us/step - loss: 0.0595 - accuracy: 0.9767\n",
      "Epoch 26/35\n",
      "28000/28000 [==============================] - 7s 252us/step - loss: 0.0593 - accuracy: 0.9769\n",
      "Epoch 27/35\n",
      "28000/28000 [==============================] - 7s 250us/step - loss: 0.0581 - accuracy: 0.9771\n",
      "Epoch 28/35\n",
      "28000/28000 [==============================] - 7s 256us/step - loss: 0.0577 - accuracy: 0.9775\n",
      "Epoch 29/35\n",
      "28000/28000 [==============================] - 7s 253us/step - loss: 0.0579 - accuracy: 0.9775\n",
      "Epoch 30/35\n",
      "28000/28000 [==============================] - 7s 256us/step - loss: 0.0574 - accuracy: 0.9776\n",
      "Epoch 31/35\n",
      "28000/28000 [==============================] - 7s 255us/step - loss: 0.0571 - accuracy: 0.9779\n",
      "Epoch 32/35\n",
      "28000/28000 [==============================] - 7s 256us/step - loss: 0.0570 - accuracy: 0.9779\n",
      "Epoch 33/35\n",
      "28000/28000 [==============================] - 7s 252us/step - loss: 0.0567 - accuracy: 0.9779\n",
      "Epoch 34/35\n",
      "28000/28000 [==============================] - 7s 260us/step - loss: 0.0563 - accuracy: 0.9783\n",
      "Epoch 35/35\n",
      "28000/28000 [==============================] - 7s 252us/step - loss: 0.0550 - accuracy: 0.9785\n",
      "Model: \"sequential_91\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_260 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_260 (Bat (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_261 (Conv2D)          (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_261 (Bat (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_262 (Conv2D)          (None, 12, 12, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_262 (Bat (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_376 (Dropout)        (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_263 (Conv2D)          (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_263 (Bat (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_264 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_264 (Bat (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_265 (Conv2D)          (None, 4, 4, 64)          102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_265 (Bat (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_377 (Dropout)        (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_266 (Conv2D)          (None, 1, 1, 128)         131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_266 (Bat (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_38 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_378 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_355 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 327,242\n",
      "Trainable params: 326,410\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n",
      "Epoch 1/1\n",
      "28000/28000 [==============================] - 14s 498us/step - loss: 0.1161 - accuracy: 0.9535\n",
      "Model: \"sequential_93\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_274 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_274 (Bat (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_275 (Conv2D)          (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_275 (Bat (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_276 (Conv2D)          (None, 12, 12, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_276 (Bat (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_382 (Dropout)        (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_277 (Conv2D)          (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_277 (Bat (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_278 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_278 (Bat (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_279 (Conv2D)          (None, 4, 4, 64)          102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_279 (Bat (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_383 (Dropout)        (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_280 (Conv2D)          (None, 1, 1, 128)         131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_280 (Bat (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_40 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_384 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_357 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 327,242\n",
      "Trainable params: 326,410\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "28000/28000 [==============================] - 14s 510us/step - loss: 0.1160 - accuracy: 0.9535\n",
      "Epoch 2/3\n",
      "28000/28000 [==============================] - 12s 430us/step - loss: 0.0774 - accuracy: 0.9695\n",
      "Epoch 3/3\n",
      "28000/28000 [==============================] - 12s 429us/step - loss: 0.0654 - accuracy: 0.9741\n",
      "Model: \"sequential_95\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_288 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_288 (Bat (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_289 (Conv2D)          (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_289 (Bat (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_290 (Conv2D)          (None, 12, 12, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_290 (Bat (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_388 (Dropout)        (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_291 (Conv2D)          (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_291 (Bat (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_292 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_292 (Bat (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_293 (Conv2D)          (None, 4, 4, 64)          102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_293 (Bat (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_389 (Dropout)        (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_294 (Conv2D)          (None, 1, 1, 128)         131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_294 (Bat (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_42 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_390 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_359 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 327,242\n",
      "Trainable params: 326,410\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "28000/28000 [==============================] - 14s 517us/step - loss: 0.1128 - accuracy: 0.9558\n",
      "Epoch 2/5\n",
      "28000/28000 [==============================] - 12s 436us/step - loss: 0.0752 - accuracy: 0.9709\n",
      "Epoch 3/5\n",
      "28000/28000 [==============================] - 12s 436us/step - loss: 0.0633 - accuracy: 0.9750\n",
      "Epoch 4/5\n",
      "28000/28000 [==============================] - 12s 438us/step - loss: 0.0582 - accuracy: 0.9772\n",
      "Epoch 5/5\n",
      "28000/28000 [==============================] - 12s 438us/step - loss: 0.0539 - accuracy: 0.9786\n",
      "Model: \"sequential_97\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_302 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_302 (Bat (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_303 (Conv2D)          (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_303 (Bat (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_304 (Conv2D)          (None, 12, 12, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_304 (Bat (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_394 (Dropout)        (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_305 (Conv2D)          (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_305 (Bat (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_306 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_306 (Bat (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_307 (Conv2D)          (None, 4, 4, 64)          102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_307 (Bat (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_395 (Dropout)        (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_308 (Conv2D)          (None, 1, 1, 128)         131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_308 (Bat (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_44 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_396 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_361 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 327,242\n",
      "Trainable params: 326,410\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n",
      "Epoch 1/15\n",
      "28000/28000 [==============================] - 14s 517us/step - loss: 0.1159 - accuracy: 0.9544\n",
      "Epoch 2/15\n",
      "28000/28000 [==============================] - 12s 434us/step - loss: 0.0762 - accuracy: 0.9698\n",
      "Epoch 3/15\n",
      "28000/28000 [==============================] - 12s 432us/step - loss: 0.0654 - accuracy: 0.9743\n",
      "Epoch 4/15\n",
      "28000/28000 [==============================] - 12s 432us/step - loss: 0.0596 - accuracy: 0.9768\n",
      "Epoch 5/15\n",
      "28000/28000 [==============================] - 12s 431us/step - loss: 0.0548 - accuracy: 0.9788\n",
      "Epoch 6/15\n",
      "28000/28000 [==============================] - 12s 432us/step - loss: 0.0510 - accuracy: 0.9803\n",
      "Epoch 7/15\n",
      "28000/28000 [==============================] - 12s 432us/step - loss: 0.0477 - accuracy: 0.9814\n",
      "Epoch 8/15\n",
      "28000/28000 [==============================] - 12s 431us/step - loss: 0.0444 - accuracy: 0.9827\n",
      "Epoch 9/15\n",
      "28000/28000 [==============================] - 12s 431us/step - loss: 0.0423 - accuracy: 0.9836\n",
      "Epoch 10/15\n",
      "28000/28000 [==============================] - 12s 432us/step - loss: 0.0397 - accuracy: 0.9845\n",
      "Epoch 11/15\n",
      "28000/28000 [==============================] - 12s 432us/step - loss: 0.0374 - accuracy: 0.9858\n",
      "Epoch 12/15\n",
      "28000/28000 [==============================] - 12s 439us/step - loss: 0.0354 - accuracy: 0.9865\n",
      "Epoch 13/15\n",
      "28000/28000 [==============================] - 12s 434us/step - loss: 0.0340 - accuracy: 0.9870\n",
      "Epoch 14/15\n",
      "28000/28000 [==============================] - 12s 434us/step - loss: 0.0321 - accuracy: 0.9881\n",
      "Epoch 15/15\n",
      "28000/28000 [==============================] - 12s 434us/step - loss: 0.0306 - accuracy: 0.9883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_99\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_316 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_316 (Bat (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_317 (Conv2D)          (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_317 (Bat (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_318 (Conv2D)          (None, 12, 12, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_318 (Bat (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_400 (Dropout)        (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_319 (Conv2D)          (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_319 (Bat (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_320 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_320 (Bat (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_321 (Conv2D)          (None, 4, 4, 64)          102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_321 (Bat (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_401 (Dropout)        (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_322 (Conv2D)          (None, 1, 1, 128)         131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_322 (Bat (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_46 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_402 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_363 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 327,242\n",
      "Trainable params: 326,410\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "28000/28000 [==============================] - 15s 527us/step - loss: 0.1149 - accuracy: 0.9545\n",
      "Epoch 2/25\n",
      "28000/28000 [==============================] - 14s 486us/step - loss: 0.0771 - accuracy: 0.9696\n",
      "Epoch 3/25\n",
      "28000/28000 [==============================] - 14s 490us/step - loss: 0.0662 - accuracy: 0.9742\n",
      "Epoch 4/25\n",
      "28000/28000 [==============================] - 13s 456us/step - loss: 0.0601 - accuracy: 0.9766\n",
      "Epoch 5/25\n",
      "28000/28000 [==============================] - 13s 464us/step - loss: 0.0552 - accuracy: 0.9786\n",
      "Epoch 6/25\n",
      "28000/28000 [==============================] - 12s 439us/step - loss: 0.0522 - accuracy: 0.9796\n",
      "Epoch 7/25\n",
      "28000/28000 [==============================] - 12s 430us/step - loss: 0.0469 - accuracy: 0.9816\n",
      "Epoch 8/25\n",
      "28000/28000 [==============================] - 13s 448us/step - loss: 0.0457 - accuracy: 0.9824\n",
      "Epoch 9/25\n",
      "28000/28000 [==============================] - 13s 477us/step - loss: 0.0425 - accuracy: 0.9838\n",
      "Epoch 10/25\n",
      "28000/28000 [==============================] - 13s 459us/step - loss: 0.0408 - accuracy: 0.9842\n",
      "Epoch 11/25\n",
      "28000/28000 [==============================] - 12s 437us/step - loss: 0.0381 - accuracy: 0.9856\n",
      "Epoch 12/25\n",
      "28000/28000 [==============================] - 13s 468us/step - loss: 0.0355 - accuracy: 0.9865\n",
      "Epoch 13/25\n",
      "28000/28000 [==============================] - 13s 457us/step - loss: 0.0336 - accuracy: 0.9873\n",
      "Epoch 14/25\n",
      "28000/28000 [==============================] - 12s 433us/step - loss: 0.0323 - accuracy: 0.9877\n",
      "Epoch 15/25\n",
      "28000/28000 [==============================] - 13s 465us/step - loss: 0.0307 - accuracy: 0.9884\n",
      "Epoch 16/25\n",
      "28000/28000 [==============================] - 14s 491us/step - loss: 0.0293 - accuracy: 0.9892\n",
      "Epoch 17/25\n",
      "28000/28000 [==============================] - 13s 459us/step - loss: 0.0277 - accuracy: 0.9895\n",
      "Epoch 18/25\n",
      "28000/28000 [==============================] - 13s 468us/step - loss: 0.0264 - accuracy: 0.9902\n",
      "Epoch 19/25\n",
      "28000/28000 [==============================] - 13s 479us/step - loss: 0.0253 - accuracy: 0.9902\n",
      "Epoch 20/25\n",
      "28000/28000 [==============================] - 13s 469us/step - loss: 0.0242 - accuracy: 0.9910\n",
      "Epoch 21/25\n",
      "28000/28000 [==============================] - 14s 497us/step - loss: 0.0228 - accuracy: 0.9915\n",
      "Epoch 22/25\n",
      "28000/28000 [==============================] - 15s 521us/step - loss: 0.0218 - accuracy: 0.9918\n",
      "Epoch 23/25\n",
      "28000/28000 [==============================] - 14s 493us/step - loss: 0.0214 - accuracy: 0.9919\n",
      "Epoch 24/25\n",
      "28000/28000 [==============================] - 13s 473us/step - loss: 0.0210 - accuracy: 0.9923\n",
      "Epoch 25/25\n",
      "28000/28000 [==============================] - 14s 497us/step - loss: 0.0193 - accuracy: 0.9928\n",
      "Model: \"sequential_101\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_330 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_330 (Bat (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_331 (Conv2D)          (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_331 (Bat (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_332 (Conv2D)          (None, 12, 12, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_332 (Bat (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_406 (Dropout)        (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_333 (Conv2D)          (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_333 (Bat (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_334 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_334 (Bat (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_335 (Conv2D)          (None, 4, 4, 64)          102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_335 (Bat (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_407 (Dropout)        (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_336 (Conv2D)          (None, 1, 1, 128)         131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_336 (Bat (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_48 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_408 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_365 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 327,242\n",
      "Trainable params: 326,410\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/35\n",
      "28000/28000 [==============================] - 16s 586us/step - loss: 0.1160 - accuracy: 0.9545\n",
      "Epoch 2/35\n",
      "28000/28000 [==============================] - 14s 488us/step - loss: 0.0755 - accuracy: 0.9704\n",
      "Epoch 3/35\n",
      "28000/28000 [==============================] - 13s 471us/step - loss: 0.0644 - accuracy: 0.9750\n",
      "Epoch 4/35\n",
      "28000/28000 [==============================] - 14s 489us/step - loss: 0.0577 - accuracy: 0.9779\n",
      "Epoch 5/35\n",
      "28000/28000 [==============================] - 14s 498us/step - loss: 0.0543 - accuracy: 0.9789\n",
      "Epoch 6/35\n",
      "28000/28000 [==============================] - 14s 508us/step - loss: 0.0500 - accuracy: 0.9806\n",
      "Epoch 7/35\n",
      "28000/28000 [==============================] - 14s 487us/step - loss: 0.0470 - accuracy: 0.9817\n",
      "Epoch 8/35\n",
      "28000/28000 [==============================] - 14s 510us/step - loss: 0.0440 - accuracy: 0.9833\n",
      "Epoch 9/35\n",
      "28000/28000 [==============================] - 15s 523us/step - loss: 0.0413 - accuracy: 0.9841\n",
      "Epoch 10/35\n",
      "28000/28000 [==============================] - 15s 520us/step - loss: 0.0383 - accuracy: 0.9852\n",
      "Epoch 11/35\n",
      "28000/28000 [==============================] - 14s 490us/step - loss: 0.0378 - accuracy: 0.9853\n",
      "Epoch 12/35\n",
      "28000/28000 [==============================] - 15s 520us/step - loss: 0.0355 - accuracy: 0.9865\n",
      "Epoch 13/35\n",
      "28000/28000 [==============================] - 14s 495us/step - loss: 0.0323 - accuracy: 0.9877\n",
      "Epoch 14/35\n",
      "28000/28000 [==============================] - 14s 483us/step - loss: 0.0314 - accuracy: 0.9882\n",
      "Epoch 15/35\n",
      "28000/28000 [==============================] - 14s 502us/step - loss: 0.0294 - accuracy: 0.9888\n",
      "Epoch 16/35\n",
      "28000/28000 [==============================] - 14s 517us/step - loss: 0.0287 - accuracy: 0.9890\n",
      "Epoch 17/35\n",
      "28000/28000 [==============================] - 14s 491us/step - loss: 0.0264 - accuracy: 0.9899\n",
      "Epoch 18/35\n",
      "28000/28000 [==============================] - 14s 511us/step - loss: 0.0264 - accuracy: 0.9901\n",
      "Epoch 19/35\n",
      "28000/28000 [==============================] - 14s 505us/step - loss: 0.0250 - accuracy: 0.9907\n",
      "Epoch 20/35\n",
      "28000/28000 [==============================] - 14s 484us/step - loss: 0.0234 - accuracy: 0.9911\n",
      "Epoch 21/35\n",
      "28000/28000 [==============================] - 13s 465us/step - loss: 0.0231 - accuracy: 0.9913\n",
      "Epoch 22/35\n",
      "28000/28000 [==============================] - 14s 508us/step - loss: 0.0219 - accuracy: 0.9917\n",
      "Epoch 23/35\n",
      "28000/28000 [==============================] - 12s 446us/step - loss: 0.0207 - accuracy: 0.9921\n",
      "Epoch 24/35\n",
      "28000/28000 [==============================] - 13s 452us/step - loss: 0.0193 - accuracy: 0.9927\n",
      "Epoch 25/35\n",
      "28000/28000 [==============================] - 13s 466us/step - loss: 0.0196 - accuracy: 0.9927\n",
      "Epoch 26/35\n",
      "28000/28000 [==============================] - 13s 453us/step - loss: 0.0193 - accuracy: 0.9927\n",
      "Epoch 27/35\n",
      "28000/28000 [==============================] - 13s 455us/step - loss: 0.0185 - accuracy: 0.9932\n",
      "Epoch 28/35\n",
      "28000/28000 [==============================] - 13s 455us/step - loss: 0.0181 - accuracy: 0.9933\n",
      "Epoch 29/35\n",
      "28000/28000 [==============================] - 13s 465us/step - loss: 0.0177 - accuracy: 0.9936\n",
      "Epoch 30/35\n",
      "28000/28000 [==============================] - 14s 490us/step - loss: 0.0163 - accuracy: 0.9939\n",
      "Epoch 31/35\n",
      "28000/28000 [==============================] - 14s 486us/step - loss: 0.0162 - accuracy: 0.9940\n",
      "Epoch 32/35\n",
      "28000/28000 [==============================] - 13s 472us/step - loss: 0.0158 - accuracy: 0.9941\n",
      "Epoch 33/35\n",
      "28000/28000 [==============================] - 13s 473us/step - loss: 0.0158 - accuracy: 0.9942\n",
      "Epoch 34/35\n",
      "28000/28000 [==============================] - 13s 480us/step - loss: 0.0151 - accuracy: 0.9943\n",
      "Epoch 35/35\n",
      "28000/28000 [==============================] - 13s 469us/step - loss: 0.0140 - accuracy: 0.9946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[                 Name  accuracy  f1_score   log_loss  precision    recall  \\\n",
       " 0  AdaBoost | 41 secs  0.674143  0.660696  11.254707   0.662973  0.675028   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.819431      0.517244  ,\n",
       "                  Name  accuracy  f1_score   log_loss  precision    recall  \\\n",
       " 0  AdaBoost | 98 secs  0.673286  0.671653  11.284312   0.682087  0.675956   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.819818      0.536792  ,\n",
       "                   Name  accuracy  f1_score   log_loss  precision    recall  \\\n",
       " 0  AdaBoost | 194 secs  0.708143  0.708621  10.080389   0.720281  0.704137   \n",
       " \n",
       "    roc_auc  prec_rec_auc  \n",
       " 0   0.8359      0.571904  ,\n",
       "                   Name  accuracy  f1_score  log_loss  precision    recall  \\\n",
       " 0  AdaBoost | 576 secs  0.730286  0.733719  9.315601   0.754045  0.728994   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.849529      0.610027  ,\n",
       "                    Name  accuracy  f1_score  log_loss  precision    recall  \\\n",
       " 0  AdaBoost | 1628 secs  0.771571  0.771118  7.889643   0.775195  0.771171   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.872892      0.648503  ,\n",
       "              Name  accuracy  f1_score  log_loss  precision    recall  \\\n",
       " 0  MLP | 1 epochs  0.814857   0.80775  6.394608   0.817601  0.816511   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.897969      0.701238  ,\n",
       "              Name  accuracy  f1_score  log_loss  precision    recall  \\\n",
       " 0  MLP | 3 epochs  0.866429  0.866259  4.613394   0.867767  0.866591   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.925871      0.774929  ,\n",
       "              Name  accuracy  f1_score  log_loss  precision    recall  \\\n",
       " 0  MLP | 5 epochs     0.855  0.851723  5.008123   0.853429  0.855015   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.919452      0.754478  ,\n",
       "               Name  accuracy  f1_score  log_loss  precision    recall  \\\n",
       " 0  MLP | 15 epochs  0.875286  0.873481  4.307479   0.876812  0.875326   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.930732      0.788716  ,\n",
       "               Name  accuracy  f1_score  log_loss  precision   recall  \\\n",
       " 0  MLP | 25 epochs  0.881714  0.880659  4.085444   0.881988  0.88049   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.933678      0.798737  ,\n",
       "               Name  accuracy  f1_score  log_loss  precision    recall  \\\n",
       " 0  MLP | 35 epochs  0.882143  0.882276  4.070642   0.883886  0.882883   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.934894      0.798801  ,\n",
       "              Name  accuracy  f1_score  log_loss  precision    recall  \\\n",
       " 0  CNN | 1 epochs  0.841143  0.843343  5.486731   0.852414  0.841688   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.912016      0.744438  ,\n",
       "              Name  accuracy  f1_score  log_loss  precision    recall  \\\n",
       " 0  CNN | 3 epochs  0.875429  0.871158  4.302545   0.872922  0.873652   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.929917       0.78534  ,\n",
       "              Name  accuracy  f1_score  log_loss  precision    recall  roc_auc  \\\n",
       " 0  CNN | 5 epochs     0.905  0.904087  3.281184   0.905481  0.903516  0.94648   \n",
       " \n",
       "    prec_rec_auc  \n",
       " 0      0.833184  ,\n",
       "               Name  accuracy  f1_score  log_loss  precision    recall  \\\n",
       " 0  CNN | 15 epochs     0.926   0.92437  2.555869    0.92639  0.924704   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.958245      0.866872  ,\n",
       "               Name  accuracy  f1_score  log_loss  precision    recall  \\\n",
       " 0  CNN | 25 epochs  0.931714  0.930311  2.358505   0.930631  0.930486   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.961458      0.875434  ,\n",
       "               Name  accuracy  f1_score  log_loss  precision    recall  \\\n",
       " 0  CNN | 35 epochs  0.940857  0.941402  2.042722   0.941658  0.941848   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.967634      0.894925  ]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests_time += [method(epochs) for method in [test_mlp_time, test_cnn_time] for epochs in [1, 3, 5, 15, 25, 35]]\n",
    "tests_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>log_loss</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>prec_rec_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost | 41 secs</td>\n",
       "      <td>0.674143</td>\n",
       "      <td>0.660696</td>\n",
       "      <td>11.254707</td>\n",
       "      <td>0.662973</td>\n",
       "      <td>0.675028</td>\n",
       "      <td>0.819431</td>\n",
       "      <td>0.517244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost | 98 secs</td>\n",
       "      <td>0.673286</td>\n",
       "      <td>0.671653</td>\n",
       "      <td>11.284312</td>\n",
       "      <td>0.682087</td>\n",
       "      <td>0.675956</td>\n",
       "      <td>0.819818</td>\n",
       "      <td>0.536792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost | 194 secs</td>\n",
       "      <td>0.708143</td>\n",
       "      <td>0.708621</td>\n",
       "      <td>10.080389</td>\n",
       "      <td>0.720281</td>\n",
       "      <td>0.704137</td>\n",
       "      <td>0.835900</td>\n",
       "      <td>0.571904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost | 576 secs</td>\n",
       "      <td>0.730286</td>\n",
       "      <td>0.733719</td>\n",
       "      <td>9.315601</td>\n",
       "      <td>0.754045</td>\n",
       "      <td>0.728994</td>\n",
       "      <td>0.849529</td>\n",
       "      <td>0.610027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AdaBoost | 1628 secs</td>\n",
       "      <td>0.771571</td>\n",
       "      <td>0.771118</td>\n",
       "      <td>7.889643</td>\n",
       "      <td>0.775195</td>\n",
       "      <td>0.771171</td>\n",
       "      <td>0.872892</td>\n",
       "      <td>0.648503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP | 1 epochs</td>\n",
       "      <td>0.814857</td>\n",
       "      <td>0.807750</td>\n",
       "      <td>6.394608</td>\n",
       "      <td>0.817601</td>\n",
       "      <td>0.816511</td>\n",
       "      <td>0.897969</td>\n",
       "      <td>0.701238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP | 3 epochs</td>\n",
       "      <td>0.866429</td>\n",
       "      <td>0.866259</td>\n",
       "      <td>4.613394</td>\n",
       "      <td>0.867767</td>\n",
       "      <td>0.866591</td>\n",
       "      <td>0.925871</td>\n",
       "      <td>0.774929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP | 5 epochs</td>\n",
       "      <td>0.855000</td>\n",
       "      <td>0.851723</td>\n",
       "      <td>5.008123</td>\n",
       "      <td>0.853429</td>\n",
       "      <td>0.855015</td>\n",
       "      <td>0.919452</td>\n",
       "      <td>0.754478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP | 15 epochs</td>\n",
       "      <td>0.875286</td>\n",
       "      <td>0.873481</td>\n",
       "      <td>4.307479</td>\n",
       "      <td>0.876812</td>\n",
       "      <td>0.875326</td>\n",
       "      <td>0.930732</td>\n",
       "      <td>0.788716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP | 25 epochs</td>\n",
       "      <td>0.881714</td>\n",
       "      <td>0.880659</td>\n",
       "      <td>4.085444</td>\n",
       "      <td>0.881988</td>\n",
       "      <td>0.880490</td>\n",
       "      <td>0.933678</td>\n",
       "      <td>0.798737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP | 35 epochs</td>\n",
       "      <td>0.882143</td>\n",
       "      <td>0.882276</td>\n",
       "      <td>4.070642</td>\n",
       "      <td>0.883886</td>\n",
       "      <td>0.882883</td>\n",
       "      <td>0.934894</td>\n",
       "      <td>0.798801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN | 1 epochs</td>\n",
       "      <td>0.841143</td>\n",
       "      <td>0.843343</td>\n",
       "      <td>5.486731</td>\n",
       "      <td>0.852414</td>\n",
       "      <td>0.841688</td>\n",
       "      <td>0.912016</td>\n",
       "      <td>0.744438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN | 3 epochs</td>\n",
       "      <td>0.875429</td>\n",
       "      <td>0.871158</td>\n",
       "      <td>4.302545</td>\n",
       "      <td>0.872922</td>\n",
       "      <td>0.873652</td>\n",
       "      <td>0.929917</td>\n",
       "      <td>0.785340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN | 5 epochs</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>0.904087</td>\n",
       "      <td>3.281184</td>\n",
       "      <td>0.905481</td>\n",
       "      <td>0.903516</td>\n",
       "      <td>0.946480</td>\n",
       "      <td>0.833184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN | 15 epochs</td>\n",
       "      <td>0.926000</td>\n",
       "      <td>0.924370</td>\n",
       "      <td>2.555869</td>\n",
       "      <td>0.926390</td>\n",
       "      <td>0.924704</td>\n",
       "      <td>0.958245</td>\n",
       "      <td>0.866872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN | 25 epochs</td>\n",
       "      <td>0.931714</td>\n",
       "      <td>0.930311</td>\n",
       "      <td>2.358505</td>\n",
       "      <td>0.930631</td>\n",
       "      <td>0.930486</td>\n",
       "      <td>0.961458</td>\n",
       "      <td>0.875434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN | 35 epochs</td>\n",
       "      <td>0.940857</td>\n",
       "      <td>0.941402</td>\n",
       "      <td>2.042722</td>\n",
       "      <td>0.941658</td>\n",
       "      <td>0.941848</td>\n",
       "      <td>0.967634</td>\n",
       "      <td>0.894925</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Name  accuracy  f1_score   log_loss  precision    recall  \\\n",
       "0    AdaBoost | 41 secs  0.674143  0.660696  11.254707   0.662973  0.675028   \n",
       "0    AdaBoost | 98 secs  0.673286  0.671653  11.284312   0.682087  0.675956   \n",
       "0   AdaBoost | 194 secs  0.708143  0.708621  10.080389   0.720281  0.704137   \n",
       "0   AdaBoost | 576 secs  0.730286  0.733719   9.315601   0.754045  0.728994   \n",
       "0  AdaBoost | 1628 secs  0.771571  0.771118   7.889643   0.775195  0.771171   \n",
       "0        MLP | 1 epochs  0.814857  0.807750   6.394608   0.817601  0.816511   \n",
       "0        MLP | 3 epochs  0.866429  0.866259   4.613394   0.867767  0.866591   \n",
       "0        MLP | 5 epochs  0.855000  0.851723   5.008123   0.853429  0.855015   \n",
       "0       MLP | 15 epochs  0.875286  0.873481   4.307479   0.876812  0.875326   \n",
       "0       MLP | 25 epochs  0.881714  0.880659   4.085444   0.881988  0.880490   \n",
       "0       MLP | 35 epochs  0.882143  0.882276   4.070642   0.883886  0.882883   \n",
       "0        CNN | 1 epochs  0.841143  0.843343   5.486731   0.852414  0.841688   \n",
       "0        CNN | 3 epochs  0.875429  0.871158   4.302545   0.872922  0.873652   \n",
       "0        CNN | 5 epochs  0.905000  0.904087   3.281184   0.905481  0.903516   \n",
       "0       CNN | 15 epochs  0.926000  0.924370   2.555869   0.926390  0.924704   \n",
       "0       CNN | 25 epochs  0.931714  0.930311   2.358505   0.930631  0.930486   \n",
       "0       CNN | 35 epochs  0.940857  0.941402   2.042722   0.941658  0.941848   \n",
       "\n",
       "    roc_auc  prec_rec_auc  \n",
       "0  0.819431      0.517244  \n",
       "0  0.819818      0.536792  \n",
       "0  0.835900      0.571904  \n",
       "0  0.849529      0.610027  \n",
       "0  0.872892      0.648503  \n",
       "0  0.897969      0.701238  \n",
       "0  0.925871      0.774929  \n",
       "0  0.919452      0.754478  \n",
       "0  0.930732      0.788716  \n",
       "0  0.933678      0.798737  \n",
       "0  0.934894      0.798801  \n",
       "0  0.912016      0.744438  \n",
       "0  0.929917      0.785340  \n",
       "0  0.946480      0.833184  \n",
       "0  0.958245      0.866872  \n",
       "0  0.961458      0.875434  \n",
       "0  0.967634      0.894925  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.concat(tests_time)\n",
    "df.to_pickle('3bb.pkl')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moc klasyfiaktora\n",
    "Tutaj trzeba przemieszać pewien % etykiet zbioru treningowego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_cnn_rob(mangling, data_sz=0.5, epochs=10):\n",
    "    name = 'CNN | {}% mangling'.format(mangling*100.0)\n",
    "    x, y = mk_dataset(data_sz)\n",
    "    y = to_categorical(y, num_classes)\n",
    "    x /= 255.0\n",
    "    x = x.reshape((x.shape[0], 28, 28, 1))\n",
    "    return mk_test(mk_cnn(epochs=epochs), name, mangling=mangling)(x, y)\n",
    "\n",
    "def test_mlp_rob(mangling, data_sz=0.5, epochs=10):\n",
    "    name = 'MLP | {}% mangling'.format(mangling*100.0)\n",
    "    x, y = mk_dataset(data_sz)\n",
    "    y = to_categorical(y, num_classes)\n",
    "    x /= 255.0\n",
    "    return mk_test(mk_mlp(epochs=epochs), name, mangling=mangling)(x, y)\n",
    "\n",
    "def test_adaboost_rob(mangling, data_sz=0.5, n=100):\n",
    "    name = 'AdaBoost | {}% mangling'.format(mangling*100.0)\n",
    "    result = mk_test(mk_adaboost(n=n), name, mangling=mangling)(*mk_dataset(data_sz))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_115 (Dense)            (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_124 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_125 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_126 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_127 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_128 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 357,066\n",
      "Trainable params: 357,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "28000/28000 [==============================] - 4s 160us/step - loss: 0.3563 - accuracy: 0.9127\n",
      "Epoch 2/10\n",
      "28000/28000 [==============================] - 4s 136us/step - loss: 0.3545 - accuracy: 0.9092\n",
      "Epoch 3/10\n",
      "28000/28000 [==============================] - 4s 141us/step - loss: 0.3327 - accuracy: 0.9152\n",
      "Epoch 4/10\n",
      "28000/28000 [==============================] - 4s 134us/step - loss: 0.3169 - accuracy: 0.9235\n",
      "Epoch 5/10\n",
      "28000/28000 [==============================] - 4s 140us/step - loss: 0.3017 - accuracy: 0.9286\n",
      "Epoch 6/10\n",
      "28000/28000 [==============================] - 4s 135us/step - loss: 0.3090 - accuracy: 0.9276\n",
      "Epoch 7/10\n",
      "28000/28000 [==============================] - 4s 153us/step - loss: 0.3015 - accuracy: 0.9269\n",
      "Epoch 8/10\n",
      "28000/28000 [==============================] - 4s 141us/step - loss: 0.3081 - accuracy: 0.9280\n",
      "Epoch 9/10\n",
      "28000/28000 [==============================] - 4s 142us/step - loss: 0.3032 - accuracy: 0.9291\n",
      "Epoch 10/10\n",
      "28000/28000 [==============================] - 4s 126us/step - loss: 0.2947 - accuracy: 0.9323\n",
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_127 (Dense)            (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_134 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_128 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_135 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_129 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_136 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_130 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_137 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_131 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_138 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_132 (Dense)            (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 357,066\n",
      "Trainable params: 357,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "28000/28000 [==============================] - 4s 149us/step - loss: 0.7782 - accuracy: 0.8643\n",
      "Epoch 2/10\n",
      "28000/28000 [==============================] - 4s 128us/step - loss: 0.7651 - accuracy: 0.8648\n",
      "Epoch 3/10\n",
      "28000/28000 [==============================] - 4s 138us/step - loss: 0.7571 - accuracy: 0.8665\n",
      "Epoch 4/10\n",
      "28000/28000 [==============================] - 5s 162us/step - loss: 0.7534 - accuracy: 0.8664\n",
      "Epoch 5/10\n",
      "28000/28000 [==============================] - 4s 146us/step - loss: 0.7482 - accuracy: 0.8671\n",
      "Epoch 6/10\n",
      "28000/28000 [==============================] - 4s 145us/step - loss: 0.7468 - accuracy: 0.8678\n",
      "Epoch 7/10\n",
      "28000/28000 [==============================] - 4s 138us/step - loss: 0.7436 - accuracy: 0.8688\n",
      "Epoch 8/10\n",
      "28000/28000 [==============================] - 4s 136us/step - loss: 0.7400 - accuracy: 0.8693\n",
      "Epoch 9/10\n",
      "28000/28000 [==============================] - 4s 129us/step - loss: 0.7410 - accuracy: 0.8699\n",
      "Epoch 10/10\n",
      "28000/28000 [==============================] - 5s 169us/step - loss: 0.7404 - accuracy: 0.8696\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_139 (Dense)            (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_144 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_140 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_145 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_141 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_146 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_142 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_147 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_143 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_148 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_144 (Dense)            (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 357,066\n",
      "Trainable params: 357,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "28000/28000 [==============================] - 5s 170us/step - loss: 1.7804 - accuracy: 0.7804\n",
      "Epoch 2/10\n",
      "28000/28000 [==============================] - 4s 143us/step - loss: 1.7741 - accuracy: 0.7802\n",
      "Epoch 3/10\n",
      "28000/28000 [==============================] - 4s 157us/step - loss: 1.7735 - accuracy: 0.7803\n",
      "Epoch 4/10\n",
      "28000/28000 [==============================] - 5s 170us/step - loss: 1.7717 - accuracy: 0.7803\n",
      "Epoch 5/10\n",
      "28000/28000 [==============================] - 4s 139us/step - loss: 1.7718 - accuracy: 0.7802\n",
      "Epoch 6/10\n",
      "28000/28000 [==============================] - 4s 157us/step - loss: 1.7706 - accuracy: 0.7801\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000/28000 [==============================] - 4s 142us/step - loss: 1.7688 - accuracy: 0.7802\n",
      "Epoch 8/10\n",
      "28000/28000 [==============================] - 4s 154us/step - loss: 1.7677 - accuracy: 0.7802\n",
      "Epoch 9/10\n",
      "28000/28000 [==============================] - 5s 167us/step - loss: 1.7678 - accuracy: 0.7803\n",
      "Epoch 10/10\n",
      "28000/28000 [==============================] - 5s 165us/step - loss: 1.7680 - accuracy: 0.7803\n",
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_151 (Dense)            (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_154 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_152 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_155 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_153 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_156 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_154 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_157 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_155 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_158 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_156 (Dense)            (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 357,066\n",
      "Trainable params: 357,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "28000/28000 [==============================] - 5s 170us/step - loss: 3.2615 - accuracy: 0.6596\n",
      "Epoch 2/10\n",
      "28000/28000 [==============================] - 4s 152us/step - loss: 3.2589 - accuracy: 0.6596\n",
      "Epoch 3/10\n",
      "28000/28000 [==============================] - 4s 143us/step - loss: 3.2582 - accuracy: 0.6596\n",
      "Epoch 4/10\n",
      "28000/28000 [==============================] - 4s 148us/step - loss: 3.2573 - accuracy: 0.6596\n",
      "Epoch 5/10\n",
      "28000/28000 [==============================] - 4s 148us/step - loss: 3.2568 - accuracy: 0.6596\n",
      "Epoch 6/10\n",
      "28000/28000 [==============================] - 4s 139us/step - loss: 3.2565 - accuracy: 0.6596\n",
      "Epoch 7/10\n",
      "28000/28000 [==============================] - 4s 138us/step - loss: 3.2568 - accuracy: 0.6596\n",
      "Epoch 8/10\n",
      "28000/28000 [==============================] - 4s 141us/step - loss: 3.2567 - accuracy: 0.6596\n",
      "Epoch 9/10\n",
      "28000/28000 [==============================] - 4s 146us/step - loss: 3.2566 - accuracy: 0.6596\n",
      "Epoch 10/10\n",
      "28000/28000 [==============================] - 4s 139us/step - loss: 3.2565 - accuracy: 0.6596\n",
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_163 (Dense)            (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_164 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_164 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_165 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_165 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_166 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_166 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_167 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_167 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_168 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_168 (Dense)            (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 357,066\n",
      "Trainable params: 357,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "28000/28000 [==============================] - 5s 179us/step - loss: 5.2156 - accuracy: 0.4973\n",
      "Epoch 2/10\n",
      "28000/28000 [==============================] - 4s 153us/step - loss: 5.2147 - accuracy: 0.4973\n",
      "Epoch 3/10\n",
      "28000/28000 [==============================] - 4s 158us/step - loss: 5.2144 - accuracy: 0.4973\n",
      "Epoch 4/10\n",
      "28000/28000 [==============================] - 4s 152us/step - loss: 5.2141 - accuracy: 0.4973\n",
      "Epoch 5/10\n",
      "28000/28000 [==============================] - 4s 160us/step - loss: 5.2139 - accuracy: 0.4973\n",
      "Epoch 6/10\n",
      "28000/28000 [==============================] - 4s 150us/step - loss: 5.2138 - accuracy: 0.4973\n",
      "Epoch 7/10\n",
      "28000/28000 [==============================] - 5s 168us/step - loss: 5.2138 - accuracy: 0.4973\n",
      "Epoch 8/10\n",
      "28000/28000 [==============================] - 4s 143us/step - loss: 5.2137 - accuracy: 0.4973\n",
      "Epoch 9/10\n",
      "28000/28000 [==============================] - 5s 176us/step - loss: 5.2136 - accuracy: 0.4973\n",
      "Epoch 10/10\n",
      "28000/28000 [==============================] - 4s 151us/step - loss: 5.2135 - accuracy: 0.4973\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_175 (Dense)            (None, 64)                50240     \n",
      "_________________________________________________________________\n",
      "dropout_174 (Dropout)        (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_176 (Dense)            (None, 128)               8320      \n",
      "_________________________________________________________________\n",
      "dropout_175 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_177 (Dense)            (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_176 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_178 (Dense)            (None, 512)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_177 (Dropout)        (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_179 (Dense)            (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_178 (Dropout)        (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_180 (Dense)            (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 357,066\n",
      "Trainable params: 357,066\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "28000/28000 [==============================] - 5s 174us/step - loss: 10.1126 - accuracy: 0.0972\n",
      "Epoch 2/10\n",
      "28000/28000 [==============================] - 4s 150us/step - loss: 10.1125 - accuracy: 0.0972\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000/28000 [==============================] - 4s 151us/step - loss: 10.1125 - accuracy: 0.0972\n",
      "Epoch 4/10\n",
      "28000/28000 [==============================] - 4s 158us/step - loss: 10.1125 - accuracy: 0.0972\n",
      "Epoch 5/10\n",
      "28000/28000 [==============================] - 4s 155us/step - loss: 10.1125 - accuracy: 0.0972\n",
      "Epoch 6/10\n",
      "28000/28000 [==============================] - 5s 169us/step - loss: 10.1125 - accuracy: 0.0972\n",
      "Epoch 7/10\n",
      "28000/28000 [==============================] - 5s 164us/step - loss: 10.1125 - accuracy: 0.0972\n",
      "Epoch 8/10\n",
      "28000/28000 [==============================] - 5s 162us/step - loss: 10.1125 - accuracy: 0.0972\n",
      "Epoch 9/10\n",
      "28000/28000 [==============================] - 5s 165us/step - loss: 10.1125 - accuracy: 0.0972\n",
      "Epoch 10/10\n",
      "28000/28000 [==============================] - 4s 150us/step - loss: 10.1125 - accuracy: 0.0972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrzej\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_92 (Conv2D)           (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_92 (Batc (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_93 (Conv2D)           (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_93 (Batc (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_94 (Conv2D)           (None, 12, 12, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_94 (Batc (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_184 (Dropout)        (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_95 (Conv2D)           (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_95 (Batc (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_96 (Conv2D)           (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_96 (Batc (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_97 (Conv2D)           (None, 4, 4, 64)          102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_97 (Batc (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_185 (Dropout)        (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_98 (Conv2D)           (None, 1, 1, 128)         131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_98 (Batc (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_186 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_187 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 327,242\n",
      "Trainable params: 326,410\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "28000/28000 [==============================] - 10s 352us/step - loss: 0.3517 - accuracy: 0.9220\n",
      "Epoch 2/10\n",
      "28000/28000 [==============================] - 8s 287us/step - loss: 0.3066 - accuracy: 0.9332\n",
      "Epoch 3/10\n",
      "28000/28000 [==============================] - 8s 303us/step - loss: 0.2961 - accuracy: 0.9359\n",
      "Epoch 4/10\n",
      "28000/28000 [==============================] - 8s 302us/step - loss: 0.2858 - accuracy: 0.9386\n",
      "Epoch 5/10\n",
      "28000/28000 [==============================] - 8s 302us/step - loss: 0.2766 - accuracy: 0.9422\n",
      "Epoch 6/10\n",
      "28000/28000 [==============================] - 8s 296us/step - loss: 0.2715 - accuracy: 0.9447\n",
      "Epoch 7/10\n",
      "28000/28000 [==============================] - 8s 292us/step - loss: 0.2689 - accuracy: 0.9466\n",
      "Epoch 8/10\n",
      "28000/28000 [==============================] - 8s 291us/step - loss: 0.2644 - accuracy: 0.9495\n",
      "Epoch 9/10\n",
      "28000/28000 [==============================] - 9s 309us/step - loss: 0.2579 - accuracy: 0.9510\n",
      "Epoch 10/10\n",
      "28000/28000 [==============================] - 9s 311us/step - loss: 0.2542 - accuracy: 0.9517\n",
      "Model: \"sequential_45\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_106 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_106 (Bat (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_107 (Conv2D)          (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_107 (Bat (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_108 (Conv2D)          (None, 12, 12, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_108 (Bat (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_190 (Dropout)        (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_109 (Conv2D)          (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_109 (Bat (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_110 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_110 (Bat (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_111 (Conv2D)          (None, 4, 4, 64)          102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_111 (Bat (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_191 (Dropout)        (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_112 (Conv2D)          (None, 1, 1, 128)         131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_112 (Bat (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_16 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_192 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_189 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 327,242\n",
      "Trainable params: 326,410\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "28000/28000 [==============================] - 10s 375us/step - loss: 0.8591 - accuracy: 0.8656\n",
      "Epoch 2/10\n",
      "28000/28000 [==============================] - 9s 309us/step - loss: 0.7807 - accuracy: 0.8672\n",
      "Epoch 3/10\n",
      "28000/28000 [==============================] - 9s 309us/step - loss: 0.7606 - accuracy: 0.8692\n",
      "Epoch 4/10\n",
      "28000/28000 [==============================] - 9s 311us/step - loss: 0.7433 - accuracy: 0.8703\n",
      "Epoch 5/10\n",
      "28000/28000 [==============================] - 9s 307us/step - loss: 0.7378 - accuracy: 0.8718\n",
      "Epoch 6/10\n",
      "28000/28000 [==============================] - 9s 311us/step - loss: 0.7356 - accuracy: 0.8713\n",
      "Epoch 7/10\n",
      "28000/28000 [==============================] - 9s 314us/step - loss: 0.7338 - accuracy: 0.8712\n",
      "Epoch 8/10\n",
      "28000/28000 [==============================] - 9s 307us/step - loss: 0.7281 - accuracy: 0.8733\n",
      "Epoch 9/10\n",
      "28000/28000 [==============================] - 9s 304us/step - loss: 0.7280 - accuracy: 0.8729\n",
      "Epoch 10/10\n",
      "28000/28000 [==============================] - 9s 304us/step - loss: 0.7240 - accuracy: 0.8736\n",
      "Model: \"sequential_47\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_120 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_120 (Bat (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_121 (Conv2D)          (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_121 (Bat (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_122 (Conv2D)          (None, 12, 12, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_122 (Bat (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_196 (Dropout)        (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_123 (Conv2D)          (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_123 (Bat (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_124 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_124 (Bat (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_125 (Conv2D)          (None, 4, 4, 64)          102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_125 (Bat (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_197 (Dropout)        (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_126 (Conv2D)          (None, 1, 1, 128)         131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_126 (Bat (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_198 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_191 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 327,242\n",
      "Trainable params: 326,410\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "28000/28000 [==============================] - 10s 355us/step - loss: 2.0212 - accuracy: 0.7757\n",
      "Epoch 2/10\n",
      "28000/28000 [==============================] - 9s 308us/step - loss: 1.8201 - accuracy: 0.7795\n",
      "Epoch 3/10\n",
      "28000/28000 [==============================] - 9s 310us/step - loss: 1.7764 - accuracy: 0.7798\n",
      "Epoch 4/10\n",
      "28000/28000 [==============================] - 8s 303us/step - loss: 1.7607 - accuracy: 0.7797\n",
      "Epoch 5/10\n",
      "28000/28000 [==============================] - 9s 306us/step - loss: 1.7566 - accuracy: 0.7797\n",
      "Epoch 6/10\n",
      "28000/28000 [==============================] - 9s 313us/step - loss: 1.7551 - accuracy: 0.7797\n",
      "Epoch 7/10\n",
      "28000/28000 [==============================] - 9s 315us/step - loss: 1.7545 - accuracy: 0.7797\n",
      "Epoch 8/10\n",
      "28000/28000 [==============================] - 9s 330us/step - loss: 1.7539 - accuracy: 0.7797\n",
      "Epoch 9/10\n",
      "28000/28000 [==============================] - 9s 311us/step - loss: 1.7538 - accuracy: 0.7796\n",
      "Epoch 10/10\n",
      "28000/28000 [==============================] - 9s 320us/step - loss: 1.7521 - accuracy: 0.7796\n",
      "Model: \"sequential_49\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_134 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_134 (Bat (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_135 (Conv2D)          (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_135 (Bat (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_136 (Conv2D)          (None, 12, 12, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_136 (Bat (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_202 (Dropout)        (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_137 (Conv2D)          (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_137 (Bat (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_138 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_138 (Bat (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_139 (Conv2D)          (None, 4, 4, 64)          102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_139 (Bat (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_203 (Dropout)        (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_140 (Conv2D)          (None, 1, 1, 128)         131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_140 (Bat (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_20 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_204 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_193 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 327,242\n",
      "Trainable params: 326,410\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "28000/28000 [==============================] - 10s 364us/step - loss: 3.5592 - accuracy: 0.6572\n",
      "Epoch 2/10\n",
      "28000/28000 [==============================] - 9s 333us/step - loss: 3.2396 - accuracy: 0.6594\n",
      "Epoch 3/10\n",
      "28000/28000 [==============================] - 9s 334us/step - loss: 3.2153 - accuracy: 0.6594\n",
      "Epoch 4/10\n",
      "28000/28000 [==============================] - 9s 334us/step - loss: 3.2127 - accuracy: 0.6594\n",
      "Epoch 5/10\n",
      "28000/28000 [==============================] - 9s 325us/step - loss: 3.2124 - accuracy: 0.6594\n",
      "Epoch 6/10\n",
      "28000/28000 [==============================] - 9s 318us/step - loss: 3.2120 - accuracy: 0.6594\n",
      "Epoch 7/10\n",
      "28000/28000 [==============================] - 9s 312us/step - loss: 3.2119 - accuracy: 0.6594\n",
      "Epoch 8/10\n",
      "28000/28000 [==============================] - 9s 329us/step - loss: 3.2115 - accuracy: 0.6594\n",
      "Epoch 9/10\n",
      "28000/28000 [==============================] - 9s 329us/step - loss: 3.2109 - accuracy: 0.6594\n",
      "Epoch 10/10\n",
      "28000/28000 [==============================] - 9s 332us/step - loss: 3.2107 - accuracy: 0.6594\n",
      "Model: \"sequential_51\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_148 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_148 (Bat (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_149 (Conv2D)          (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_149 (Bat (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_150 (Conv2D)          (None, 12, 12, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_150 (Bat (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_208 (Dropout)        (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_151 (Conv2D)          (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_151 (Bat (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_152 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_152 (Bat (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_153 (Conv2D)          (None, 4, 4, 64)          102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_153 (Bat (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_209 (Dropout)        (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_154 (Conv2D)          (None, 1, 1, 128)         131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_154 (Bat (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_22 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_210 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_195 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 327,242\n",
      "Trainable params: 326,410\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "28000/28000 [==============================] - 12s 415us/step - loss: 5.6989 - accuracy: 0.4972\n",
      "Epoch 2/10\n",
      "28000/28000 [==============================] - 10s 347us/step - loss: 5.2493 - accuracy: 0.4988\n",
      "Epoch 3/10\n",
      "28000/28000 [==============================] - 10s 339us/step - loss: 5.1915 - accuracy: 0.4990\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28000/28000 [==============================] - 10s 347us/step - loss: 5.1805 - accuracy: 0.4990\n",
      "Epoch 5/10\n",
      "28000/28000 [==============================] - 9s 339us/step - loss: 5.1793 - accuracy: 0.4990\n",
      "Epoch 6/10\n",
      "28000/28000 [==============================] - 10s 351us/step - loss: 5.1793 - accuracy: 0.4990\n",
      "Epoch 7/10\n",
      "28000/28000 [==============================] - 10s 354us/step - loss: 5.1798 - accuracy: 0.4990\n",
      "Epoch 8/10\n",
      "28000/28000 [==============================] - 9s 334us/step - loss: 5.1798 - accuracy: 0.4990\n",
      "Epoch 9/10\n",
      "28000/28000 [==============================] - 9s 329us/step - loss: 5.1795 - accuracy: 0.4990\n",
      "Epoch 10/10\n",
      "28000/28000 [==============================] - 9s 328us/step - loss: 5.1790 - accuracy: 0.4990\n",
      "Model: \"sequential_53\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_162 (Conv2D)          (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "batch_normalization_162 (Bat (None, 26, 26, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_163 (Conv2D)          (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "batch_normalization_163 (Bat (None, 24, 24, 32)        128       \n",
      "_________________________________________________________________\n",
      "conv2d_164 (Conv2D)          (None, 12, 12, 32)        25632     \n",
      "_________________________________________________________________\n",
      "batch_normalization_164 (Bat (None, 12, 12, 32)        128       \n",
      "_________________________________________________________________\n",
      "dropout_214 (Dropout)        (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_165 (Conv2D)          (None, 10, 10, 64)        18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_165 (Bat (None, 10, 10, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_166 (Conv2D)          (None, 8, 8, 64)          36928     \n",
      "_________________________________________________________________\n",
      "batch_normalization_166 (Bat (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv2d_167 (Conv2D)          (None, 4, 4, 64)          102464    \n",
      "_________________________________________________________________\n",
      "batch_normalization_167 (Bat (None, 4, 4, 64)          256       \n",
      "_________________________________________________________________\n",
      "dropout_215 (Dropout)        (None, 4, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_168 (Conv2D)          (None, 1, 1, 128)         131200    \n",
      "_________________________________________________________________\n",
      "batch_normalization_168 (Bat (None, 1, 1, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten_24 (Flatten)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_216 (Dropout)        (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_197 (Dense)            (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 327,242\n",
      "Trainable params: 326,410\n",
      "Non-trainable params: 832\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "28000/28000 [==============================] - 11s 398us/step - loss: 10.8188 - accuracy: 0.1005\n",
      "Epoch 2/10\n",
      "28000/28000 [==============================] - 9s 338us/step - loss: 10.0587 - accuracy: 0.1005\n",
      "Epoch 3/10\n",
      "28000/28000 [==============================] - 10s 340us/step - loss: 10.0237 - accuracy: 0.1005\n",
      "Epoch 4/10\n",
      "28000/28000 [==============================] - 9s 337us/step - loss: 10.0227 - accuracy: 0.1005\n",
      "Epoch 5/10\n",
      "28000/28000 [==============================] - 9s 339us/step - loss: 10.0227 - accuracy: 0.1005\n",
      "Epoch 6/10\n",
      "28000/28000 [==============================] - 10s 343us/step - loss: 10.0227 - accuracy: 0.1005\n",
      "Epoch 7/10\n",
      "28000/28000 [==============================] - 10s 361us/step - loss: 10.0227 - accuracy: 0.1005\n",
      "Epoch 8/10\n",
      "28000/28000 [==============================] - 10s 356us/step - loss: 10.0227 - accuracy: 0.1005\n",
      "Epoch 9/10\n",
      "28000/28000 [==============================] - 9s 337us/step - loss: 10.0254 - accuracy: 0.1005\n",
      "Epoch 10/10\n",
      "28000/28000 [==============================] - 10s 348us/step - loss: 10.0227 - accuracy: 0.1005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andrzej\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score</th>\n",
       "      <th>log_loss</th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>roc_auc</th>\n",
       "      <th>prec_rec_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP | 1.0% mangling</td>\n",
       "      <td>0.727714</td>\n",
       "      <td>0.706931</td>\n",
       "      <td>9.404415</td>\n",
       "      <td>0.735001</td>\n",
       "      <td>0.726581</td>\n",
       "      <td>0.848189</td>\n",
       "      <td>0.594932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP | 5.0% mangling</td>\n",
       "      <td>0.690714</td>\n",
       "      <td>0.675783</td>\n",
       "      <td>10.682350</td>\n",
       "      <td>0.739160</td>\n",
       "      <td>0.688164</td>\n",
       "      <td>0.826941</td>\n",
       "      <td>0.533126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP | 15.0% mangling</td>\n",
       "      <td>0.776429</td>\n",
       "      <td>0.747428</td>\n",
       "      <td>7.721884</td>\n",
       "      <td>0.737148</td>\n",
       "      <td>0.775088</td>\n",
       "      <td>0.875103</td>\n",
       "      <td>0.662083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP | 30.0% mangling</td>\n",
       "      <td>0.726429</td>\n",
       "      <td>0.694929</td>\n",
       "      <td>9.448822</td>\n",
       "      <td>0.743245</td>\n",
       "      <td>0.729286</td>\n",
       "      <td>0.849453</td>\n",
       "      <td>0.600243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP | 50.0% mangling</td>\n",
       "      <td>0.774000</td>\n",
       "      <td>0.759196</td>\n",
       "      <td>7.805763</td>\n",
       "      <td>0.777235</td>\n",
       "      <td>0.773175</td>\n",
       "      <td>0.874046</td>\n",
       "      <td>0.651529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MLP | 100.0% mangling</td>\n",
       "      <td>0.097857</td>\n",
       "      <td>0.017827</td>\n",
       "      <td>31.158910</td>\n",
       "      <td>0.009786</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN | 1.0% mangling</td>\n",
       "      <td>0.862571</td>\n",
       "      <td>0.859230</td>\n",
       "      <td>4.746615</td>\n",
       "      <td>0.862418</td>\n",
       "      <td>0.861175</td>\n",
       "      <td>0.922965</td>\n",
       "      <td>0.766735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN | 5.0% mangling</td>\n",
       "      <td>0.829714</td>\n",
       "      <td>0.818981</td>\n",
       "      <td>5.881460</td>\n",
       "      <td>0.833432</td>\n",
       "      <td>0.828163</td>\n",
       "      <td>0.904632</td>\n",
       "      <td>0.718065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN | 15.0% mangling</td>\n",
       "      <td>0.803571</td>\n",
       "      <td>0.793650</td>\n",
       "      <td>6.784403</td>\n",
       "      <td>0.807451</td>\n",
       "      <td>0.806404</td>\n",
       "      <td>0.892313</td>\n",
       "      <td>0.685420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN | 30.0% mangling</td>\n",
       "      <td>0.807429</td>\n",
       "      <td>0.805027</td>\n",
       "      <td>6.651182</td>\n",
       "      <td>0.810626</td>\n",
       "      <td>0.808755</td>\n",
       "      <td>0.893671</td>\n",
       "      <td>0.690830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN | 50.0% mangling</td>\n",
       "      <td>0.662000</td>\n",
       "      <td>0.641011</td>\n",
       "      <td>11.674106</td>\n",
       "      <td>0.694999</td>\n",
       "      <td>0.667991</td>\n",
       "      <td>0.815212</td>\n",
       "      <td>0.548557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNN | 100.0% mangling</td>\n",
       "      <td>0.113857</td>\n",
       "      <td>0.071812</td>\n",
       "      <td>30.606290</td>\n",
       "      <td>0.080872</td>\n",
       "      <td>0.114829</td>\n",
       "      <td>0.508219</td>\n",
       "      <td>0.104232</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Name  accuracy  f1_score   log_loss  precision    recall  \\\n",
       "0    MLP | 1.0% mangling  0.727714  0.706931   9.404415   0.735001  0.726581   \n",
       "0    MLP | 5.0% mangling  0.690714  0.675783  10.682350   0.739160  0.688164   \n",
       "0   MLP | 15.0% mangling  0.776429  0.747428   7.721884   0.737148  0.775088   \n",
       "0   MLP | 30.0% mangling  0.726429  0.694929   9.448822   0.743245  0.729286   \n",
       "0   MLP | 50.0% mangling  0.774000  0.759196   7.805763   0.777235  0.773175   \n",
       "0  MLP | 100.0% mangling  0.097857  0.017827  31.158910   0.009786  0.100000   \n",
       "0    CNN | 1.0% mangling  0.862571  0.859230   4.746615   0.862418  0.861175   \n",
       "0    CNN | 5.0% mangling  0.829714  0.818981   5.881460   0.833432  0.828163   \n",
       "0   CNN | 15.0% mangling  0.803571  0.793650   6.784403   0.807451  0.806404   \n",
       "0   CNN | 30.0% mangling  0.807429  0.805027   6.651182   0.810626  0.808755   \n",
       "0   CNN | 50.0% mangling  0.662000  0.641011  11.674106   0.694999  0.667991   \n",
       "0  CNN | 100.0% mangling  0.113857  0.071812  30.606290   0.080872  0.114829   \n",
       "\n",
       "    roc_auc  prec_rec_auc  \n",
       "0  0.848189      0.594932  \n",
       "0  0.826941      0.533126  \n",
       "0  0.875103      0.662083  \n",
       "0  0.849453      0.600243  \n",
       "0  0.874046      0.651529  \n",
       "0  0.500000      0.100000  \n",
       "0  0.922965      0.766735  \n",
       "0  0.904632      0.718065  \n",
       "0  0.892313      0.685420  \n",
       "0  0.893671      0.690830  \n",
       "0  0.815212      0.548557  \n",
       "0  0.508219      0.104232  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#robustness_test = [method(man) for method in [test_adaboost_rob, test_mlp_rob, test_cnn_rob] for man in [0.01, 0.05, 0.15, 0.3]]\n",
    "robustness_test = [method(man) for method in [test_mlp_rob, test_cnn_rob] for man in [0.0, 0.01, 0.05, 0.15, 0.3, 0.5, 1.0]]\n",
    "pd.concat(robustness_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[                  Name  accuracy  f1_score  log_loss  precision    recall  \\\n",
       " 0  MLP | 1.0% mangling  0.727714  0.706931  9.404415   0.735001  0.726581   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.848189      0.594932  ,\n",
       "                   Name  accuracy  f1_score  log_loss  precision    recall  \\\n",
       " 0  MLP | 5.0% mangling  0.690714  0.675783  10.68235    0.73916  0.688164   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.826941      0.533126  ,\n",
       "                    Name  accuracy  f1_score  log_loss  precision    recall  \\\n",
       " 0  MLP | 15.0% mangling  0.776429  0.747428  7.721884   0.737148  0.775088   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.875103      0.662083  ,\n",
       "                    Name  accuracy  f1_score  log_loss  precision    recall  \\\n",
       " 0  MLP | 30.0% mangling  0.726429  0.694929  9.448822   0.743245  0.729286   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.849453      0.600243  ,\n",
       "                    Name  accuracy  f1_score  log_loss  precision    recall  \\\n",
       " 0  MLP | 50.0% mangling     0.774  0.759196  7.805763   0.777235  0.773175   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.874046      0.651529  ,\n",
       "                     Name  accuracy  f1_score  log_loss  precision  recall  \\\n",
       " 0  MLP | 100.0% mangling  0.097857  0.017827  31.15891   0.009786     0.1   \n",
       " \n",
       "    roc_auc  prec_rec_auc  \n",
       " 0      0.5           0.1  ,\n",
       "                   Name  accuracy  f1_score  log_loss  precision    recall  \\\n",
       " 0  CNN | 1.0% mangling  0.862571   0.85923  4.746615   0.862418  0.861175   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.922965      0.766735  ,\n",
       "                   Name  accuracy  f1_score  log_loss  precision    recall  \\\n",
       " 0  CNN | 5.0% mangling  0.829714  0.818981   5.88146   0.833432  0.828163   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.904632      0.718065  ,\n",
       "                    Name  accuracy  f1_score  log_loss  precision    recall  \\\n",
       " 0  CNN | 15.0% mangling  0.803571   0.79365  6.784403   0.807451  0.806404   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.892313       0.68542  ,\n",
       "                    Name  accuracy  f1_score  log_loss  precision    recall  \\\n",
       " 0  CNN | 30.0% mangling  0.807429  0.805027  6.651182   0.810626  0.808755   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.893671       0.69083  ,\n",
       "                    Name  accuracy  f1_score   log_loss  precision    recall  \\\n",
       " 0  CNN | 50.0% mangling     0.662  0.641011  11.674106   0.694999  0.667991   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.815212      0.548557  ,\n",
       "                     Name  accuracy  f1_score  log_loss  precision    recall  \\\n",
       " 0  CNN | 100.0% mangling  0.113857  0.071812  30.60629   0.080872  0.114829   \n",
       " \n",
       "     roc_auc  prec_rec_auc  \n",
       " 0  0.508219      0.104232  ]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.conrobustness_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
